{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FunctionalDyBM Demo\n",
    "===================\n",
    "- Author: Hiroshi Kajino\n",
    "- Date: Sep 27, 2016.\n",
    "- (C) Copyright IBM Corp. 2016\n",
    "\n",
    "FunctionalDyBM (F-DyBM) models the dynamics of a function $f^{[t]}(x)$, where $x\\in\\mathcal{X}$ is a feature vector in a feature space, and $t\\in\\mathbb{N}$ is a discrete time step.\n",
    "For example, $f^{[t]}(x)$ can represent the temperature at location $x$ and time step $t$.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "For each time step $t$,\n",
    "\n",
    "1. it receives finite observations of a pattern, $[f^{[t]}(x_1^{[t]}),\\dots,f^{[t]}(x_{N^{[t]}}^{[t]})]$,\n",
    "\n",
    "1. it learns the model parameters using `learn_one_step(pattern, loc)` method, where `loc` corresponds to $[x_1^{[t]},\\dots,x_{N^{[t]}}^{[t]}]$ and `pattern` corresponds to $[f^{[t]}(x_1^{[t]}),\\dots,f^{[t]}(x_{N^{[t]}}^{[t]})]$, and\n",
    "\n",
    "1. predicts the next pattern $f^{[t+1]}(x)$ at any location $x\\in\\mathcal{X}$ using `predict_next(loc)` method, where `loc` corresponds to a set of locations in which the next patterns are calculated.\n",
    "\n",
    "Model descriptions and initialization\n",
    "-------------------------------------\n",
    "F-DyBM has the following two memory units, and utilize them to predict the next pattern $f^{[t+1]}(x)$.\n",
    "\n",
    "- queue stores raw patterns\n",
    "\n",
    "- eligibility traces store some statistics of all the previous patterns. If we use `insert_to_etrace=\"w_delay\"`, a pattern popped from the queue will be inserted to eligibility traces (thus delayed by `delay`), and if we use `insert_to_etrace=\"wo_delay\"`, a pattern enqueued to the queue is also inserted into eligibility traces.\n",
    "\n",
    "The weight parameters used to predict the next pattern from the memory units are stored in `self.variables` and will be learned on-the-fly using `learn_one_step` method.\n",
    "\n",
    "F- DyBM can be initialized using the following hyperparameters:\n",
    "\n",
    "- `dim` is the dimension of the feature space $\\mathcal{X}$.\n",
    "\n",
    "- `anc_points` is an array of shape `(n_anc, dim)`.\n",
    "\n",
    "- `delay` determines the length of the queue (which will be `delay-1`).\n",
    "\n",
    "- `decay_rates` is a list of eligibility traces' parameters.\n",
    "\n",
    "- `noise_var` determines the noise variance on the pattern.\n",
    "\n",
    "- `ker_paras` sets the kernel function $K(x, x^{\\prime})$, used for function approximation.\n",
    "\n",
    "- `insert_to_etrace` determines the definition of eligibility traces, as discussed above.\n",
    "\n",
    "- `learning_rate` sets the learning rate of SGD.\n",
    "\n",
    "Example\n",
    "-------\n",
    "In the following example, we let F-DyBM learn `test_func`. At each time step, we calculate RMSE between the actual pattern given by `test_func` and the prediction given by F-DyBM and present the score in every 100 steps, and at the end, we show the actual pattern and the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:\t RSME = 0.804297057773843\n",
      "step 100:\t RSME = 0.19065979348680756\n",
      "step 200:\t RSME = 0.08098709018367681\n",
      "step 300:\t RSME = 0.04750983481968885\n",
      "step 400:\t RSME = 0.03489134381583352\n",
      "step 500:\t RSME = 0.03669342287636237\n",
      "step 600:\t RSME = 0.03691558073285102\n",
      "step 700:\t RSME = 0.01656613738495161\n",
      "step 800:\t RSME = 0.039154285932145\n",
      "step 900:\t RSME = 0.030231793172077692\n",
      "\n",
      "pattern = [ 0.22816552  0.46928338  0.40465998  0.82039651  0.69958197  0.72838946\n",
      "  0.46086176  0.12740303  0.37587426  0.09493412  0.64909052  0.51408783\n",
      "  0.83356479  0.51357757  0.44394982  0.66815925  0.49099194  0.6314766\n",
      "  0.51187462  0.39545874  0.79438388  0.9288989   0.05273392  0.95175743\n",
      " -0.35581896  0.80602032  0.69375366  0.24147853  0.79609136  0.5606667\n",
      "  0.39235362 -0.15255081  0.77692577  0.21062653 -0.32438     0.85193157\n",
      "  0.33286599  0.85778826  0.23943005  0.83246809  0.96467168 -0.22380548\n",
      " -0.03368148  0.30272629  0.8218343   0.25702916  0.50423556  0.39985679\n",
      " -0.36830519 -0.04733531  0.70668148  0.70533733  0.40891569  0.21510256\n",
      "  0.00298809  0.45190865  0.53993095  0.27493229  0.42772455 -0.0397178\n",
      "  0.4124046   0.82301996  0.43827617 -0.20355792  0.16807818  0.0699583\n",
      "  0.43316091  0.12669137  0.66111232  0.20974915 -0.32287104  0.34824671\n",
      "  0.32697991  0.49487165  0.66908842 -0.41193512 -0.23864637  0.40988633\n",
      "  0.66140511  0.40666975  0.3780057   0.4834181   0.00581134  0.63226338\n",
      "  0.17920632  0.95580171  0.61952287  0.48606423  0.60328591  0.083733\n",
      "  0.58891508  0.52152718 -0.17305296  0.86813706  0.38624729  0.57291852\n",
      "  0.36715611  0.67916658 -0.29014262  0.55384654]\n",
      "pred = [ 0.24548064  0.45469457  0.41275089  0.83392217  0.65400937  0.75523484\n",
      "  0.47914909  0.10704212  0.34723888  0.09903598  0.6797946   0.53855347\n",
      "  0.84520807  0.52524572  0.43040162  0.64317086  0.4693467   0.62205365\n",
      "  0.53283662  0.38714784  0.80700687  0.91004485  0.05518617  0.91172914\n",
      " -0.31170572  0.80181438  0.73476146  0.22942605  0.83135405  0.55034986\n",
      "  0.4010956  -0.16152871  0.81601074  0.19873421 -0.30027453  0.87450213\n",
      "  0.33431127  0.8757042   0.22871511  0.86057553  0.90853148 -0.21318091\n",
      " -0.03491831  0.31188601  0.84524112  0.24986124  0.46147514  0.40769004\n",
      " -0.32759868 -0.03558201  0.73152129  0.66060569  0.41981042  0.23196437\n",
      " -0.0182414   0.43378374  0.54202491  0.26712218  0.44151277 -0.06185142\n",
      "  0.40180245  0.84595322  0.44165695 -0.20159373  0.16148128  0.0525297\n",
      "  0.4321608   0.12439701  0.68215848  0.20685216 -0.2996258   0.35129625\n",
      "  0.32527808  0.5163105   0.70061605 -0.35530894 -0.23823041  0.40720891\n",
      "  0.6791423   0.36117236  0.38403039  0.4858905   0.00334528  0.61907995\n",
      "  0.17535762  0.91669667  0.59222792  0.49231104  0.6189944   0.05917583\n",
      "  0.60783757  0.51244456 -0.17846164  0.85099046  0.3811633   0.59611389\n",
      "  0.3742872   0.64398699 -0.26847592  0.58300494]\n"
     ]
    }
   ],
   "source": [
    "from pydybm.time_series.functional_dybm import FunctionalDyBM\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "\n",
    "MAX_ITER=1000 # number of time steps\n",
    "dim = 2 # dimension of feature space\n",
    "n_obs = 100 # number of observations at each time step\n",
    "n_anc = 10 # number of anchor points, which are used for defining basis functions\n",
    "delay = 3 # a pattern will be fed to eligibility traces with this delay\n",
    "decay_rates = [0.2, 0.9] # parameters of eligibility traces. in this case, we have two eligibility traces\n",
    "\n",
    "def test_func(loc,t):\n",
    "    \"\"\"\n",
    "    loc : array, shape (n_obs, dim)\n",
    "        each row corresponds to a coordinate of each observation point.\n",
    "    t : int\n",
    "        time step\n",
    "    \"\"\"\n",
    "    return np.sin(loc.sum(axis=1) + t/10.0) + 0.00001 * np.random.randn(loc.shape[0])\n",
    "\n",
    "\n",
    "# initialize anchor points and models\n",
    "anc = np.random.uniform(low=0.0, high=1.0, size=(n_anc, dim))\n",
    "model = FunctionalDyBM(dim=dim, anc_points=anc, delay=delay, decay_rates=decay_rates,\n",
    "                      noise_var=1.0, ker_paras={\"ker_type\":\"rbf\", \"gamma\":1.0},\n",
    "                      insert_to_etrace=\"w_delay\", learning_rate=0.0001)\n",
    "\n",
    "# for each time step t, \n",
    "# (i) we randomly generate observation points, `loc`, and observations of a functional pattern at the points, `pattern`,\n",
    "# (ii) compute RMSE between the actual pattern and prediction by the model, and\n",
    "# (ii) we update the parameters as well as the internal states of F-DyBM using fit().\n",
    "for t in xrange(MAX_ITER):\n",
    "    loc = np.random.uniform(0.0, 1.0, (n_obs, dim))\n",
    "    pattern = test_func(loc, t)\n",
    "    if t%100 == 0:\n",
    "        print(\"step {}:\\t RSME = {}\".format(t, model.compute_RMSE(pattern, loc)))\n",
    "    model.learn_one_step(pattern, loc)\n",
    "\n",
    "# the pattern at time step MAX_ITER and the predicted pattern by the model will be presented.\n",
    "loc = np.random.uniform(0.0, 1.0, (n_obs, dim))\n",
    "pattern = test_func(loc, MAX_ITER)\n",
    "print(\"\\npattern = {}\".format(pattern))\n",
    "print(\"pred = {}\".format(model.predict_next(loc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

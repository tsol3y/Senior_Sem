{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-27T23:42:41.698676Z",
     "start_time": "2017-10-27T23:42:41.695152Z"
    }
   },
   "source": [
    "This notebook is a little easier for beginners because it uses pytorch. You need to clone a repo to get it working:\n",
    "\n",
    "```sh\n",
    "# you need this repo, so clone it\n",
    "git clone https://github.com/wassname/DeepRL.git\n",
    "cd DeepRL\n",
    "git reset --hard aeae2c5d585e5853dc638968b1f090eb60abd351\n",
    "cd ..\n",
    "mkdir data log evaluation_log\n",
    "```\n",
    "\n",
    "This contains some minor modifications from https://github.com/ShangtongZhang/DeepRL.git\n",
    "\n",
    "The notebook tries DPPG with the [EIIE model](https://arxiv.org/pdf/1706.10059.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:51:12.766648Z",
     "start_time": "2018-02-18T03:51:12.763971Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also uncommented reward normalization in DDPG_agent.py#L64 because otherwise my small reward les to large Q's, inf losses, and NaN actions and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.118663Z",
     "start_time": "2018-02-18T06:06:07.274323Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:__main__ logger started.\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# numeric\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.150314Z",
     "start_time": "2018-02-18T06:06:08.122827Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "os.sys.path.append(os.path.abspath('DeepRL'))\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T06:04:03.382623Z",
     "start_time": "2018-01-15T06:04:03.312027Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.172640Z",
     "start_time": "2018-02-18T06:06:08.152417Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "window_length = 50\n",
    "steps = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.209252Z",
     "start_time": "2018-02-18T06:06:08.176664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/pytorch-DDPG/pytorch-DDPG-EIIE-action-crypto-20200423_21-48-39.model\n"
     ]
    }
   ],
   "source": [
    "# save dir\n",
    "import datetime\n",
    "ts = datetime.datetime.utcnow().strftime('%Y%m%d_%H-%M-%S')\n",
    "\n",
    "save_path = './outputs/pytorch-DDPG/pytorch-DDPG-EIIE-action-crypto-%s.model' % ts\n",
    "print(save_path)\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:08.984401Z",
     "start_time": "2018-02-18T06:06:08.212090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ryano\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir runs/ddpg-20200423_21-48-39\n"
     ]
    }
   ],
   "source": [
    "# setup tensorboard logging\n",
    "from tensorboard_logger import configure, log_value\n",
    "tag = 'ddpg-' + ts\n",
    "print('tensorboard --logdir '+\"runs/\" + tag)\n",
    "try:\n",
    "    configure(\"runs/\" + tag)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:25:35.539014Z",
     "start_time": "2018-01-23T04:25:32.708434Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.100223Z",
     "start_time": "2018-02-18T06:06:08.986130Z"
    }
   },
   "outputs": [],
   "source": [
    "from rl_portfolio_management.environments.portfolio import PortfolioEnv\n",
    "from rl_portfolio_management.util import MDD, sharpe, softmax\n",
    "from rl_portfolio_management.wrappers import SoftmaxActions, TransposeHistory, ConcatStates\n",
    "\n",
    "df_train = pd.read_hdf('./data/poloniex_30m.hf',key='train')\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T06:17:58.191717Z",
     "start_time": "2018-01-15T06:17:56.636432Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.175657Z",
     "start_time": "2018-02-18T06:06:09.112261Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "class DeepRLWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.render_on_reset = False\n",
    "        \n",
    "        self.state_dim = self.observation_space.shape\n",
    "        self.action_dim = self.action_space.shape[0]\n",
    "        \n",
    "        self.name = 'PortfolioEnv'\n",
    "        self.success_threshold = 2\n",
    "        \n",
    "    def normalize_state(self, state):\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info =self.env.step(action)\n",
    "        reward*=1e4 # often reward scaling is important sooo...\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def reset(self):        \n",
    "        # here's a roundabout way to get it to plot on reset\n",
    "        if self.render_on_reset: \n",
    "            self.env.render('notebook')\n",
    "\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.290527Z",
     "start_time": "2018-02-18T06:06:09.195655Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryano\\Desktop\\CSCI Cap\\Cap-Res-Proj\\ddpg-stock-trading-code\\rl_portfolio_management\\environments\\portfolio.py:52: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = df.as_matrix().reshape(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4, 51, 3), (4, 51, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def task_fn():\n",
    "    env = PortfolioEnv(df=df_train, steps=steps, output_mode='EIIE')\n",
    "    env = TransposeHistory(env)\n",
    "    env = ConcatStates(env)\n",
    "    env = SoftmaxActions(env)\n",
    "    env = DeepRLWrapper(env)\n",
    "    return env\n",
    "\n",
    "def task_fn_test():\n",
    "    env = PortfolioEnv(df=df_test, steps=steps, output_mode='EIIE')\n",
    "    env = TransposeHistory(env)\n",
    "    env = ConcatStates(env)\n",
    "    env = SoftmaxActions(env)\n",
    "    env = DeepRLWrapper(env)\n",
    "    return env\n",
    "    \n",
    "# sanity check\n",
    "task = task_fn()\n",
    "task.reset().shape, task.step(task.action_space.sample())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.450771Z",
     "start_time": "2018-02-18T06:06:09.292152Z"
    }
   },
   "outputs": [],
   "source": [
    "# load\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "def save_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    save_file = 'data/%s-%s-model-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    agent.save(save_file)\n",
    "    print(save_file)\n",
    "    \n",
    "\n",
    "def load_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    save_file = 'data/%s-%s-model-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    new_states = pickle.load(open(save_file, 'rb'))\n",
    "    states = agent.worker_network.load_state_dict(new_states)\n",
    "\n",
    "\n",
    "def load_stats_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    online_stats_file = 'data/%s-%s-online-stats-%s.bin' % (\n",
    "                    agent_type, config.tag, agent.task.name)\n",
    "    try:\n",
    "        steps, rewards = pickle.load(open(online_stats_file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        steps =[]\n",
    "        rewards=[]\n",
    "    df_online = pd.DataFrame(np.array([steps, rewards]).T, columns=['steps','rewards'])\n",
    "    if len(df_online):\n",
    "        df_online['step'] = df_online['steps'].cumsum()\n",
    "        df_online.index.name = 'episodes'\n",
    "    \n",
    "    stats_file = 'data/%s-%s-all-stats-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "\n",
    "    try:\n",
    "        stats = pickle.load(open(stats_file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        stats = {}\n",
    "    df = pd.DataFrame(stats[\"test_rewards\"], columns=['rewards'])\n",
    "    if len(df):\n",
    "#         df[\"steps\"]=range(len(df))*50\n",
    "\n",
    "        df.index.name = 'episodes'\n",
    "    return df_online, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:09.751094Z",
     "start_time": "2018-02-18T06:06:09.453781Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from DeepRL.agent import ProximalPolicyOptimization\n",
    "from DeepRL.network import DisjointActorCriticNet #, DeterministicActorNet, DeterministicCriticNet\n",
    "from DeepRL.component import GaussianPolicy, HighDimActionReplay, OrnsteinUhlenbeckProcess\n",
    "from DeepRL.utils import Config, Logger\n",
    "import gym\n",
    "import torch\n",
    "gym.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T05:24:46.546070Z",
     "start_time": "2018-01-15T05:24:46.542443Z"
    }
   },
   "source": [
    "# Alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:10.618760Z",
     "start_time": "2018-02-18T06:06:09.753415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modified from https://github.com/ShangtongZhang/DeepRL to log to tensorboard\n",
    "\n",
    "from DeepRL.utils.normalizer import Normalizer\n",
    "\n",
    "null_normaliser = lambda x:x\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.worker_network = config.network_fn()\n",
    "        self.target_network = config.network_fn()\n",
    "        self.target_network.load_state_dict(self.worker_network.state_dict())\n",
    "        self.actor_opt = config.actor_optimizer_fn(self.worker_network.actor.parameters())\n",
    "        self.critic_opt = config.critic_optimizer_fn(self.worker_network.critic.parameters())\n",
    "        self.replay = config.replay_fn()\n",
    "        self.random_process = config.random_process_fn()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.state_normalizer = Normalizer(self.task.state_dim) # null_normaliser # \n",
    "        self.reward_normalizer = Normalizer(1)\n",
    "\n",
    "    def soft_update(self, target, src):\n",
    "        for target_param, param in zip(target.parameters(), src.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.config.target_network_mix) +\n",
    "                                    param.data * self.config.target_network_mix)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            torch.save(self.worker_network.state_dict(), f)\n",
    "\n",
    "    def episode(self, deterministic=False, video_recorder=None):\n",
    "        self.random_process.reset_states()\n",
    "        state = self.task.reset()\n",
    "        state = self.state_normalizer(state)\n",
    "\n",
    "        config = self.config\n",
    "        actor = self.worker_network.actor\n",
    "        critic = self.worker_network.critic\n",
    "        target_actor = self.target_network.actor\n",
    "        target_critic = self.target_network.critic\n",
    "\n",
    "        steps = 0\n",
    "        total_reward = 0.0\n",
    "        while True:\n",
    "            actor.eval()\n",
    "            action = actor.predict(np.stack([state])).flatten()\n",
    "            if not deterministic:\n",
    "                action += self.random_process.sample()\n",
    "            next_state, reward, done, info = self.task.step(action)\n",
    "            if video_recorder is not None:\n",
    "                video_recorder.capture_frame()\n",
    "            done = (done or (config.max_episode_length and steps >= config.max_episode_length))\n",
    "            next_state = self.state_normalizer(next_state) * config.reward_scaling\n",
    "            total_reward += reward\n",
    "            \n",
    "            # tensorboard logging\n",
    "            prefix = 'test_' if deterministic else ''\n",
    "            log_value(prefix + 'reward', reward, self.total_steps)\n",
    "#             log_value(prefix + 'action', action, steps)\n",
    "            log_value('memory_size', self.replay.size(), self.total_steps)     \n",
    "            for key in info:\n",
    "                log_value(key, info[key], self.total_steps)     \n",
    "            \n",
    "            reward = self.reward_normalizer(reward)\n",
    "\n",
    "            if not deterministic:\n",
    "                self.replay.feed([state, action, reward, next_state, int(done)])\n",
    "                self.total_steps += 1\n",
    "\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            if not deterministic and self.replay.size() >= config.min_memory_size:\n",
    "                self.worker_network.train()\n",
    "                experiences = self.replay.sample()\n",
    "                states, actions, rewards, next_states, terminals = experiences\n",
    "                q_next = target_critic.predict(next_states, target_actor.predict(next_states))\n",
    "                terminals = critic.to_torch_variable(terminals).unsqueeze(1)\n",
    "                rewards = critic.to_torch_variable(rewards).unsqueeze(1)\n",
    "                q_next = config.discount * q_next * (1 - terminals)\n",
    "                q_next.add_(rewards)\n",
    "                q_next = q_next.detach()\n",
    "                q = critic.predict(states, actions)\n",
    "                critic_loss = self.criterion(q, q_next)\n",
    "\n",
    "                critic.zero_grad()\n",
    "                self.critic_opt.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                if config.gradient_clip:\n",
    "                    grad_critic = nn.utils.clip_grad_norm(self.worker_network.parameters(), config.gradient_clip)\n",
    "                self.critic_opt.step()\n",
    "\n",
    "                actions = actor.predict(states, False)\n",
    "                var_actions = Variable(actions.data, requires_grad=True)\n",
    "                q = critic.predict(states, var_actions)\n",
    "                q.backward(torch.ones(q.size()))\n",
    "\n",
    "                actor.zero_grad()\n",
    "                self.actor_opt.zero_grad()\n",
    "                actions.backward(-var_actions.grad.data)\n",
    "                if config.gradient_clip:\n",
    "                    grad_actor = nn.utils.clip_grad_norm(self.worker_network.parameters(), config.gradient_clip)\n",
    "                self.actor_opt.step()\n",
    "                \n",
    "                # tensorboard logging\n",
    "                log_value('critic_loss', critic_loss.cpu().data.numpy().squeeze(), self.total_steps)\n",
    "                log_value('loss_action', -q.sum(), self.total_steps)\n",
    "                if config.gradient_clip:\n",
    "                    log_value('grad_critic', grad_critic, self.total_steps)\n",
    "                    log_value('grad_actor', grad_actor, self.total_steps)\n",
    "\n",
    "                self.soft_update(self.target_network, self.worker_network)\n",
    "\n",
    "        return total_reward, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T06:33:40.472361Z",
     "start_time": "2018-01-15T06:33:40.450577Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:10.648063Z",
     "start_time": "2018-02-18T06:06:10.620822Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:10.685519Z",
     "start_time": "2018-02-18T06:06:10.649679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 51, 3), 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.state_dim, task.action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:11.251457Z",
     "start_time": "2018-02-18T06:06:10.688106Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from DeepRL.network.base_network import BasicNet\n",
    "\n",
    "class DeterministicActorNet(nn.Module, BasicNet):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 action_gate,\n",
    "                 action_scale,\n",
    "                 gpu=False,\n",
    "                 batch_norm=False,\n",
    "                 non_linear=F.relu):\n",
    "        super(DeterministicActorNet, self).__init__()\n",
    "\n",
    "        stride_time = state_dim[1] - 1 - 2 #\n",
    "        features = task.state_dim[0]\n",
    "        h0 = 2\n",
    "        h1 = 30\n",
    "        self.conv1 = nn.Conv2d(features, h0, (3, 1))\n",
    "        self.conv2 = nn.Conv2d(h0, h1, (stride_time, 1), stride=(stride_time, 1))\n",
    "        self.conv3 = nn.Conv2d((h1+1), 1, (1, 1))\n",
    "\n",
    "        self.action_scale = action_scale\n",
    "        self.action_gate = action_gate\n",
    "        self.non_linear = non_linear\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(h0)\n",
    "            self.bn2 = nn.BatchNorm1d(h1)\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        BasicNet.__init__(self, None, gpu, False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_torch_variable(x)\n",
    "        \n",
    "        w0 = x[:,:1,:1,:] # weights from last step \n",
    "        x = x[:,:,1:,:]\n",
    "        \n",
    "        phi0 = self.non_linear(self.conv1(x))\n",
    "        if self.batch_norm:\n",
    "            phi0 = self.bn1(phi0)\n",
    "        phi1 = self.non_linear(self.conv2(phi0))\n",
    "        h = torch.cat([phi1,w0], 1)\n",
    "        if self.batch_norm:\n",
    "            h = self.bn2(h)\n",
    "        \n",
    "        action = self.conv3(h)\n",
    "        \n",
    "        # add cash_bias before we softmax\n",
    "        cash_bias_int = 0\n",
    "        cash_bias = self.to_torch_variable(torch.ones(action.size())[:,:,:,:1] * cash_bias_int)\n",
    "        action = torch.cat([cash_bias, action], -1)\n",
    "        \n",
    "        batch_size = action.size()[0]\n",
    "        action = action.view((batch_size,-1))\n",
    "        if self.action_gate:\n",
    "            action = self.action_scale * self.action_gate(action)\n",
    "        return action\n",
    "\n",
    "    def predict(self, x, to_numpy=True):\n",
    "        y = self.forward(x)\n",
    "        if to_numpy:\n",
    "            y = y.cpu().data.numpy()\n",
    "        return y\n",
    "\n",
    "class DeterministicCriticNet(nn.Module, BasicNet):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 gpu=False,\n",
    "                 batch_norm=False,\n",
    "                 non_linear=F.relu):\n",
    "        super(DeterministicCriticNet, self).__init__()\n",
    "        stride_time = state_dim[1] - 1 - 2 #\n",
    "        self.features = features = task.state_dim[0]\n",
    "        h0=2\n",
    "        h1=20\n",
    "        self.action = actions = action_dim -1\n",
    "        self.conv1 = nn.Conv2d(features, h0, (3, 1))\n",
    "        self.conv2 = nn.Conv2d(h0, h1, (stride_time, 1), stride=(stride_time, 1))\n",
    "        self.layer3 = nn.Linear((h1+2)*actions, 1)\n",
    "        self.non_linear = non_linear\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(h0)\n",
    "            self.bn2 = nn.BatchNorm1d(h1)\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        BasicNet.__init__(self, None, gpu, False)\n",
    "\n",
    "\n",
    "    def forward(self, x, action):\n",
    "        x = self.to_torch_variable(x)\n",
    "        action = self.to_torch_variable(action)[:,None,None,:-1] # remove cash bias\n",
    "        \n",
    "        w0 = x[:,:1,:1,:] # weights from last step \n",
    "        x = x[:,:,1:,:]\n",
    "        \n",
    "        phi0 = self.non_linear(self.conv1(x))\n",
    "        if self.batch_norm:\n",
    "            phi0 = self.bn1(phi0)\n",
    "        phi1 = self.non_linear(self.conv2(phi0))\n",
    "        h = torch.cat([phi1,w0,action], 1)\n",
    "        if self.batch_norm:\n",
    "            h = self.bn2(h)\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        action = self.layer3(h.view((batch_size,-1)))\n",
    "        return action\n",
    "\n",
    "    def predict(self, x, action):\n",
    "        return self.forward(x, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:09:10.244296Z",
     "start_time": "2018-02-18T03:09:10.218211Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-10T04:53:05.213318Z",
     "start_time": "2018-02-10T04:53:05.209185Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T06:06:11.430338Z",
     "start_time": "2018-02-18T06:06:11.253821Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryano\\Desktop\\CSCI Cap\\Cap-Res-Proj\\ddpg-stock-trading-code\\rl_portfolio_management\\environments\\portfolio.py:52: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = df.as_matrix().reshape(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DDPGAgent at 0x29e8d503b70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "config.task_fn = task_fn\n",
    "task = config.task_fn()\n",
    "config.actor_network_fn = lambda: DeterministicActorNet(\n",
    "    task.state_dim, task.action_dim, action_gate=None, action_scale=1.0, non_linear=F.relu, batch_norm=False, gpu=False)\n",
    "config.critic_network_fn = lambda: DeterministicCriticNet(\n",
    "    task.state_dim, task.action_dim, non_linear=F.relu, batch_norm=False, gpu=False)\n",
    "config.network_fn = lambda: DisjointActorCriticNet(config.actor_network_fn, config.critic_network_fn)\n",
    "config.actor_optimizer_fn = lambda params: torch.optim.Adam(params, lr=4e-5)\n",
    "config.critic_optimizer_fn =\\\n",
    "    lambda params: torch.optim.Adam(params, lr=5e-4, weight_decay=0.001)\n",
    "config.replay_fn = lambda: HighDimActionReplay(memory_size=600, batch_size=64)\n",
    "config.random_process_fn = \\\n",
    "    lambda: OrnsteinUhlenbeckProcess(size=task.action_dim, theta=0.15, sigma=0.2, sigma_min=0.00002, n_steps_annealing=10000)\n",
    "config.discount = 0.0\n",
    "\n",
    "config.min_memory_size = 50\n",
    "config.target_network_mix = 0.001\n",
    "config.max_steps = 300000\n",
    "config.max_episode_length = 3000 \n",
    "config.target_network_mix = 0.01\n",
    "config.noise_decay_interval = 100000\n",
    "config.gradient_clip = 20\n",
    "config.min_epsilon = 0.1\n",
    "\n",
    "# Many papers have found rewards scaling to be an important parameter. But while they focus on the scaling factor\n",
    "# I think they should focus on the end variance with a range of 200-400. e.g. https://arxiv.org/pdf/1709.06560.pdf\n",
    "# Hard to tell for sure without experiments to prove it\n",
    "config.reward_scaling = 1000\n",
    "\n",
    "config.test_interval = 10 # ORIGINALLY\n",
    "# config.test_interval = 2 # TODO: Remove (quick test)\n",
    "config.test_repetitions = 1\n",
    "config.save_interval = 40 # ORIGINALLY\n",
    "# config.save_interval = 4 # TODO: Remove (quick test)\n",
    "config.logger = Logger('./log', gym.logger)\n",
    "config.tag = tag\n",
    "agent = DDPGAgent(config)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:13.422024Z",
     "start_time": "2018-02-18T06:06:11.432034Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryano\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "C:\\Users\\Ryano\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\ipykernel_launcher.py:107: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0423 17:48:51.617868 17664 misc.py:27] episode 1, reward -3.039298, avg reward -3.039298, total steps 128, episode step 128\n",
      "[2020-04-23 17:48:51,617] episode 1, reward -3.039298, avg reward -3.039298, total steps 128, episode step 128\n",
      "I0423 17:48:58.523808 17664 misc.py:27] episode 2, reward -4.202390, avg reward -3.620844, total steps 256, episode step 128\n",
      "[2020-04-23 17:48:58,523] episode 2, reward -4.202390, avg reward -3.620844, total steps 256, episode step 128\n",
      "I0423 17:49:05.608614 17664 misc.py:27] episode 3, reward -4.488160, avg reward -3.909950, total steps 384, episode step 128\n",
      "[2020-04-23 17:49:05,608] episode 3, reward -4.488160, avg reward -3.909950, total steps 384, episode step 128\n",
      "I0423 17:49:14.109929 17664 misc.py:27] episode 4, reward 2.910715, avg reward -2.204783, total steps 512, episode step 128\n",
      "[2020-04-23 17:49:14,109] episode 4, reward 2.910715, avg reward -2.204783, total steps 512, episode step 128\n",
      "I0423 17:49:22.608917 17664 misc.py:27] episode 5, reward -1.655644, avg reward -2.094955, total steps 640, episode step 128\n",
      "[2020-04-23 17:49:22,608] episode 5, reward -1.655644, avg reward -2.094955, total steps 640, episode step 128\n",
      "I0423 17:49:30.538294 17664 misc.py:27] episode 6, reward -3.456709, avg reward -2.321914, total steps 768, episode step 128\n",
      "[2020-04-23 17:49:30,538] episode 6, reward -3.456709, avg reward -2.321914, total steps 768, episode step 128\n",
      "I0423 17:49:37.323122 17664 misc.py:27] episode 7, reward -1.878309, avg reward -2.258542, total steps 896, episode step 128\n",
      "[2020-04-23 17:49:37,323] episode 7, reward -1.878309, avg reward -2.258542, total steps 896, episode step 128\n",
      "I0423 17:49:45.179648 17664 misc.py:27] episode 8, reward -2.202553, avg reward -2.251544, total steps 1024, episode step 128\n",
      "[2020-04-23 17:49:45,179] episode 8, reward -2.202553, avg reward -2.251544, total steps 1024, episode step 128\n",
      "I0423 17:49:53.106930 17664 misc.py:27] episode 9, reward -0.547600, avg reward -2.062217, total steps 1152, episode step 128\n",
      "[2020-04-23 17:49:53,106] episode 9, reward -0.547600, avg reward -2.062217, total steps 1152, episode step 128\n",
      "I0423 17:50:01.161283 17664 misc.py:27] episode 10, reward -1.482476, avg reward -2.004242, total steps 1280, episode step 128\n",
      "[2020-04-23 17:50:01,161] episode 10, reward -1.482476, avg reward -2.004242, total steps 1280, episode step 128\n",
      "I0423 17:50:01.166652 17664 misc.py:47] Testing...\n",
      "[2020-04-23 17:50:01,166] Testing...\n",
      "I0423 17:50:01.958359 17664 misc.py:55] Avg reward 8.249015(0.000000)\n",
      "[2020-04-23 17:50:01,958] Avg reward 8.249015(0.000000)\n",
      "I0423 17:50:09.865626 17664 misc.py:27] episode 11, reward -1.912533, avg reward -1.995905, total steps 1408, episode step 128\n",
      "[2020-04-23 17:50:09,865] episode 11, reward -1.912533, avg reward -1.995905, total steps 1408, episode step 128\n",
      "I0423 17:50:17.517854 17664 misc.py:27] episode 12, reward -4.689981, avg reward -2.220412, total steps 1536, episode step 128\n",
      "[2020-04-23 17:50:17,517] episode 12, reward -4.689981, avg reward -2.220412, total steps 1536, episode step 128\n",
      "I0423 17:50:25.312964 17664 misc.py:27] episode 13, reward -1.140287, avg reward -2.137325, total steps 1664, episode step 128\n",
      "[2020-04-23 17:50:25,312] episode 13, reward -1.140287, avg reward -2.137325, total steps 1664, episode step 128\n",
      "I0423 17:50:32.556987 17664 misc.py:27] episode 14, reward -4.015114, avg reward -2.271453, total steps 1792, episode step 128\n",
      "[2020-04-23 17:50:32,556] episode 14, reward -4.015114, avg reward -2.271453, total steps 1792, episode step 128\n",
      "I0423 17:50:39.943573 17664 misc.py:27] episode 15, reward -1.173315, avg reward -2.198244, total steps 1920, episode step 128\n",
      "[2020-04-23 17:50:39,943] episode 15, reward -1.173315, avg reward -2.198244, total steps 1920, episode step 128\n",
      "I0423 17:50:47.352429 17664 misc.py:27] episode 16, reward -2.260578, avg reward -2.202140, total steps 2048, episode step 128\n",
      "[2020-04-23 17:50:47,352] episode 16, reward -2.260578, avg reward -2.202140, total steps 2048, episode step 128\n",
      "I0423 17:50:55.255800 17664 misc.py:27] episode 17, reward 1.180382, avg reward -2.003168, total steps 2176, episode step 128\n",
      "[2020-04-23 17:50:55,255] episode 17, reward 1.180382, avg reward -2.003168, total steps 2176, episode step 128\n",
      "I0423 17:51:03.424375 17664 misc.py:27] episode 18, reward 3.859734, avg reward -1.677451, total steps 2304, episode step 128\n",
      "[2020-04-23 17:51:03,424] episode 18, reward 3.859734, avg reward -1.677451, total steps 2304, episode step 128\n",
      "I0423 17:51:11.389373 17664 misc.py:27] episode 19, reward -4.235761, avg reward -1.812099, total steps 2432, episode step 128\n",
      "[2020-04-23 17:51:11,389] episode 19, reward -4.235761, avg reward -1.812099, total steps 2432, episode step 128\n",
      "I0423 17:51:18.575725 17664 misc.py:27] episode 20, reward 19.210791, avg reward -0.760954, total steps 2560, episode step 128\n",
      "[2020-04-23 17:51:18,575] episode 20, reward 19.210791, avg reward -0.760954, total steps 2560, episode step 128\n",
      "I0423 17:51:18.584852 17664 misc.py:47] Testing...\n",
      "[2020-04-23 17:51:18,584] Testing...\n",
      "I0423 17:51:19.525250 17664 misc.py:55] Avg reward 0.301011(0.000000)\n",
      "[2020-04-23 17:51:19,525] Avg reward 0.301011(0.000000)\n",
      "I0423 17:51:27.410658 17664 misc.py:27] episode 21, reward 4.486221, avg reward -0.511089, total steps 2688, episode step 128\n",
      "[2020-04-23 17:51:27,410] episode 21, reward 4.486221, avg reward -0.511089, total steps 2688, episode step 128\n",
      "I0423 17:51:35.503170 17664 misc.py:27] episode 22, reward -6.842748, avg reward -0.798892, total steps 2816, episode step 128\n",
      "[2020-04-23 17:51:35,503] episode 22, reward -6.842748, avg reward -0.798892, total steps 2816, episode step 128\n",
      "I0423 17:51:43.385988 17664 misc.py:27] episode 23, reward 5.650606, avg reward -0.518479, total steps 2944, episode step 128\n",
      "[2020-04-23 17:51:43,385] episode 23, reward 5.650606, avg reward -0.518479, total steps 2944, episode step 128\n",
      "I0423 17:51:51.246123 17664 misc.py:27] episode 24, reward -3.075287, avg reward -0.625012, total steps 3072, episode step 128\n",
      "[2020-04-23 17:51:51,246] episode 24, reward -3.075287, avg reward -0.625012, total steps 3072, episode step 128\n",
      "I0423 17:51:59.394003 17664 misc.py:27] episode 25, reward -0.424484, avg reward -0.616991, total steps 3200, episode step 128\n",
      "[2020-04-23 17:51:59,394] episode 25, reward -0.424484, avg reward -0.616991, total steps 3200, episode step 128\n",
      "I0423 17:52:07.288870 17664 misc.py:27] episode 26, reward -6.397569, avg reward -0.839321, total steps 3328, episode step 128\n",
      "[2020-04-23 17:52:07,288] episode 26, reward -6.397569, avg reward -0.839321, total steps 3328, episode step 128\n",
      "I0423 17:52:15.311135 17664 misc.py:27] episode 27, reward -4.102459, avg reward -0.960178, total steps 3456, episode step 128\n",
      "[2020-04-23 17:52:15,311] episode 27, reward -4.102459, avg reward -0.960178, total steps 3456, episode step 128\n",
      "I0423 17:52:23.432942 17664 misc.py:27] episode 28, reward 0.230982, avg reward -0.917637, total steps 3584, episode step 128\n",
      "[2020-04-23 17:52:23,432] episode 28, reward 0.230982, avg reward -0.917637, total steps 3584, episode step 128\n",
      "I0423 17:52:31.119601 17664 misc.py:27] episode 29, reward -2.594449, avg reward -0.975458, total steps 3712, episode step 128\n",
      "[2020-04-23 17:52:31,119] episode 29, reward -2.594449, avg reward -0.975458, total steps 3712, episode step 128\n",
      "I0423 17:52:39.193175 17664 misc.py:27] episode 30, reward -2.769947, avg reward -1.035274, total steps 3840, episode step 128\n",
      "[2020-04-23 17:52:39,193] episode 30, reward -2.769947, avg reward -1.035274, total steps 3840, episode step 128\n",
      "I0423 17:52:39.199262 17664 misc.py:47] Testing...\n",
      "[2020-04-23 17:52:39,199] Testing...\n",
      "I0423 17:52:40.027111 17664 misc.py:55] Avg reward -6.803361(0.000000)\n",
      "[2020-04-23 17:52:40,027] Avg reward -6.803361(0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 17:52:47.689260 17664 misc.py:27] episode 31, reward -1.141515, avg reward -1.038701, total steps 3968, episode step 128\n",
      "[2020-04-23 17:52:47,689] episode 31, reward -1.141515, avg reward -1.038701, total steps 3968, episode step 128\n",
      "I0423 17:52:55.413120 17664 misc.py:27] episode 32, reward -1.039056, avg reward -1.038712, total steps 4096, episode step 128\n",
      "[2020-04-23 17:52:55,413] episode 32, reward -1.039056, avg reward -1.038712, total steps 4096, episode step 128\n",
      "I0423 17:53:03.094568 17664 misc.py:27] episode 33, reward -1.612786, avg reward -1.056108, total steps 4224, episode step 128\n",
      "[2020-04-23 17:53:03,094] episode 33, reward -1.612786, avg reward -1.056108, total steps 4224, episode step 128\n",
      "I0423 17:53:10.306246 17664 misc.py:27] episode 34, reward 6.352451, avg reward -0.838210, total steps 4352, episode step 128\n",
      "[2020-04-23 17:53:10,306] episode 34, reward 6.352451, avg reward -0.838210, total steps 4352, episode step 128\n",
      "I0423 17:53:17.381762 17664 misc.py:27] episode 35, reward 8.651268, avg reward -0.567082, total steps 4480, episode step 128\n",
      "[2020-04-23 17:53:17,381] episode 35, reward 8.651268, avg reward -0.567082, total steps 4480, episode step 128\n",
      "I0423 17:53:24.330411 17664 misc.py:27] episode 36, reward -0.955467, avg reward -0.577870, total steps 4608, episode step 128\n",
      "[2020-04-23 17:53:24,330] episode 36, reward -0.955467, avg reward -0.577870, total steps 4608, episode step 128\n",
      "I0423 17:53:32.917957 17664 misc.py:27] episode 37, reward -8.762389, avg reward -0.799073, total steps 4736, episode step 128\n",
      "[2020-04-23 17:53:32,917] episode 37, reward -8.762389, avg reward -0.799073, total steps 4736, episode step 128\n",
      "I0423 17:53:40.347257 17664 misc.py:27] episode 38, reward -5.785436, avg reward -0.930293, total steps 4864, episode step 128\n",
      "[2020-04-23 17:53:40,347] episode 38, reward -5.785436, avg reward -0.930293, total steps 4864, episode step 128\n",
      "I0423 17:53:47.614758 17664 misc.py:27] episode 39, reward 5.231539, avg reward -0.772298, total steps 4992, episode step 128\n",
      "[2020-04-23 17:53:47,614] episode 39, reward 5.231539, avg reward -0.772298, total steps 4992, episode step 128\n",
      "I0423 17:53:55.076258 17664 misc.py:27] episode 40, reward 12.518318, avg reward -0.440032, total steps 5120, episode step 128\n",
      "[2020-04-23 17:53:55,076] episode 40, reward 12.518318, avg reward -0.440032, total steps 5120, episode step 128\n",
      "I0423 17:53:55.085075 17664 misc.py:47] Testing...\n",
      "[2020-04-23 17:53:55,085] Testing...\n",
      "I0423 17:53:55.848432 17664 misc.py:55] Avg reward -0.463928(0.000000)\n",
      "[2020-04-23 17:53:55,848] Avg reward -0.463928(0.000000)\n",
      "I0423 17:54:02.720095 17664 misc.py:27] episode 41, reward 6.293489, avg reward -0.275800, total steps 5248, episode step 128\n",
      "[2020-04-23 17:54:02,720] episode 41, reward 6.293489, avg reward -0.275800, total steps 5248, episode step 128\n",
      "I0423 17:54:10.337719 17664 misc.py:27] episode 42, reward 0.663591, avg reward -0.253434, total steps 5376, episode step 128\n",
      "[2020-04-23 17:54:10,337] episode 42, reward 0.663591, avg reward -0.253434, total steps 5376, episode step 128\n",
      "I0423 17:54:17.633584 17664 misc.py:27] episode 43, reward -0.071001, avg reward -0.249191, total steps 5504, episode step 128\n",
      "[2020-04-23 17:54:17,633] episode 43, reward -0.071001, avg reward -0.249191, total steps 5504, episode step 128\n",
      "I0423 17:54:25.435246 17664 misc.py:27] episode 44, reward -7.692216, avg reward -0.418351, total steps 5632, episode step 128\n",
      "[2020-04-23 17:54:25,435] episode 44, reward -7.692216, avg reward -0.418351, total steps 5632, episode step 128\n",
      "I0423 17:54:32.073235 17664 misc.py:27] episode 45, reward 7.626091, avg reward -0.239585, total steps 5760, episode step 128\n",
      "[2020-04-23 17:54:32,073] episode 45, reward 7.626091, avg reward -0.239585, total steps 5760, episode step 128\n",
      "I0423 17:54:38.535089 17664 misc.py:27] episode 46, reward -2.570565, avg reward -0.290259, total steps 5888, episode step 128\n",
      "[2020-04-23 17:54:38,535] episode 46, reward -2.570565, avg reward -0.290259, total steps 5888, episode step 128\n",
      "I0423 17:54:45.071939 17664 misc.py:27] episode 47, reward -5.639908, avg reward -0.404081, total steps 6016, episode step 128\n",
      "[2020-04-23 17:54:45,071] episode 47, reward -5.639908, avg reward -0.404081, total steps 6016, episode step 128\n",
      "I0423 17:54:52.132604 17664 misc.py:27] episode 48, reward 0.547279, avg reward -0.384261, total steps 6144, episode step 128\n",
      "[2020-04-23 17:54:52,132] episode 48, reward 0.547279, avg reward -0.384261, total steps 6144, episode step 128\n",
      "I0423 17:54:59.506754 17664 misc.py:27] episode 49, reward -2.428061, avg reward -0.425971, total steps 6272, episode step 128\n",
      "[2020-04-23 17:54:59,506] episode 49, reward -2.428061, avg reward -0.425971, total steps 6272, episode step 128\n",
      "I0423 17:55:06.965420 17664 misc.py:27] episode 50, reward -0.387371, avg reward -0.425199, total steps 6400, episode step 128\n",
      "[2020-04-23 17:55:06,965] episode 50, reward -0.387371, avg reward -0.425199, total steps 6400, episode step 128\n",
      "I0423 17:55:06.972500 17664 misc.py:47] Testing...\n",
      "[2020-04-23 17:55:06,972] Testing...\n",
      "I0423 17:55:07.899646 17664 misc.py:55] Avg reward 3.822339(0.000000)\n",
      "[2020-04-23 17:55:07,899] Avg reward 3.822339(0.000000)\n",
      "I0423 17:55:15.267650 17664 misc.py:27] episode 51, reward -4.548208, avg reward -0.506043, total steps 6528, episode step 128\n",
      "[2020-04-23 17:55:15,267] episode 51, reward -4.548208, avg reward -0.506043, total steps 6528, episode step 128\n",
      "I0423 17:55:22.822867 17664 misc.py:27] episode 52, reward -2.957173, avg reward -0.553180, total steps 6656, episode step 128\n",
      "[2020-04-23 17:55:22,822] episode 52, reward -2.957173, avg reward -0.553180, total steps 6656, episode step 128\n",
      "I0423 17:55:30.218426 17664 misc.py:27] episode 53, reward -1.118755, avg reward -0.563851, total steps 6784, episode step 128\n",
      "[2020-04-23 17:55:30,218] episode 53, reward -1.118755, avg reward -0.563851, total steps 6784, episode step 128\n",
      "I0423 17:55:37.698656 17664 misc.py:27] episode 54, reward -4.240752, avg reward -0.631942, total steps 6912, episode step 128\n",
      "[2020-04-23 17:55:37,698] episode 54, reward -4.240752, avg reward -0.631942, total steps 6912, episode step 128\n",
      "I0423 17:55:45.575686 17664 misc.py:27] episode 55, reward 0.083587, avg reward -0.618932, total steps 7040, episode step 128\n",
      "[2020-04-23 17:55:45,575] episode 55, reward 0.083587, avg reward -0.618932, total steps 7040, episode step 128\n",
      "I0423 17:55:53.192785 17664 misc.py:27] episode 56, reward -8.022139, avg reward -0.751132, total steps 7168, episode step 128\n",
      "[2020-04-23 17:55:53,192] episode 56, reward -8.022139, avg reward -0.751132, total steps 7168, episode step 128\n",
      "I0423 17:56:00.939829 17664 misc.py:27] episode 57, reward -0.688629, avg reward -0.750036, total steps 7296, episode step 128\n",
      "[2020-04-23 17:56:00,939] episode 57, reward -0.688629, avg reward -0.750036, total steps 7296, episode step 128\n",
      "I0423 17:56:08.522607 17664 misc.py:27] episode 58, reward -2.778796, avg reward -0.785014, total steps 7424, episode step 128\n",
      "[2020-04-23 17:56:08,522] episode 58, reward -2.778796, avg reward -0.785014, total steps 7424, episode step 128\n",
      "I0423 17:56:15.964898 17664 misc.py:27] episode 59, reward 6.337552, avg reward -0.664293, total steps 7552, episode step 128\n",
      "[2020-04-23 17:56:15,964] episode 59, reward 6.337552, avg reward -0.664293, total steps 7552, episode step 128\n",
      "I0423 17:56:23.331453 17664 misc.py:27] episode 60, reward 5.573309, avg reward -0.560333, total steps 7680, episode step 128\n",
      "[2020-04-23 17:56:23,331] episode 60, reward 5.573309, avg reward -0.560333, total steps 7680, episode step 128\n",
      "I0423 17:56:23.336392 17664 misc.py:47] Testing...\n",
      "[2020-04-23 17:56:23,336] Testing...\n",
      "I0423 17:56:24.107782 17664 misc.py:55] Avg reward 1.586192(0.000000)\n",
      "[2020-04-23 17:56:24,107] Avg reward 1.586192(0.000000)\n",
      "I0423 17:56:31.992248 17664 misc.py:27] episode 61, reward -2.211142, avg reward -0.587395, total steps 7808, episode step 128\n",
      "[2020-04-23 17:56:31,992] episode 61, reward -2.211142, avg reward -0.587395, total steps 7808, episode step 128\n",
      "I0423 17:56:39.882081 17664 misc.py:27] episode 62, reward 7.005788, avg reward -0.464925, total steps 7936, episode step 128\n",
      "[2020-04-23 17:56:39,882] episode 62, reward 7.005788, avg reward -0.464925, total steps 7936, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 17:56:49.054306 17664 misc.py:27] episode 63, reward -5.230516, avg reward -0.540569, total steps 8064, episode step 128\n",
      "[2020-04-23 17:56:49,054] episode 63, reward -5.230516, avg reward -0.540569, total steps 8064, episode step 128\n",
      "I0423 17:56:56.532104 17664 misc.py:27] episode 64, reward -0.512746, avg reward -0.540134, total steps 8192, episode step 128\n",
      "[2020-04-23 17:56:56,532] episode 64, reward -0.512746, avg reward -0.540134, total steps 8192, episode step 128\n",
      "I0423 17:57:04.019936 17664 misc.py:27] episode 65, reward -2.871350, avg reward -0.575999, total steps 8320, episode step 128\n",
      "[2020-04-23 17:57:04,019] episode 65, reward -2.871350, avg reward -0.575999, total steps 8320, episode step 128\n",
      "I0423 17:57:11.299302 17664 misc.py:27] episode 66, reward -0.286554, avg reward -0.571613, total steps 8448, episode step 128\n",
      "[2020-04-23 17:57:11,299] episode 66, reward -0.286554, avg reward -0.571613, total steps 8448, episode step 128\n",
      "I0423 17:57:18.814082 17664 misc.py:27] episode 67, reward -1.850645, avg reward -0.590704, total steps 8576, episode step 128\n",
      "[2020-04-23 17:57:18,814] episode 67, reward -1.850645, avg reward -0.590704, total steps 8576, episode step 128\n",
      "I0423 17:57:26.271521 17664 misc.py:27] episode 68, reward 4.189250, avg reward -0.520410, total steps 8704, episode step 128\n",
      "[2020-04-23 17:57:26,271] episode 68, reward 4.189250, avg reward -0.520410, total steps 8704, episode step 128\n",
      "I0423 17:57:33.896681 17664 misc.py:27] episode 69, reward 2.467523, avg reward -0.477107, total steps 8832, episode step 128\n",
      "[2020-04-23 17:57:33,896] episode 69, reward 2.467523, avg reward -0.477107, total steps 8832, episode step 128\n",
      "I0423 17:57:41.607821 17664 misc.py:27] episode 70, reward 2.543202, avg reward -0.433959, total steps 8960, episode step 128\n",
      "[2020-04-23 17:57:41,607] episode 70, reward 2.543202, avg reward -0.433959, total steps 8960, episode step 128\n",
      "I0423 17:57:41.612486 17664 misc.py:47] Testing...\n",
      "[2020-04-23 17:57:41,612] Testing...\n",
      "I0423 17:57:42.427156 17664 misc.py:55] Avg reward -3.399944(0.000000)\n",
      "[2020-04-23 17:57:42,427] Avg reward -3.399944(0.000000)\n",
      "I0423 17:57:50.515880 17664 misc.py:27] episode 71, reward -6.066290, avg reward -0.513288, total steps 9088, episode step 128\n",
      "[2020-04-23 17:57:50,515] episode 71, reward -6.066290, avg reward -0.513288, total steps 9088, episode step 128\n",
      "I0423 17:57:58.579761 17664 misc.py:27] episode 72, reward -5.877500, avg reward -0.587791, total steps 9216, episode step 128\n",
      "[2020-04-23 17:57:58,579] episode 72, reward -5.877500, avg reward -0.587791, total steps 9216, episode step 128\n",
      "I0423 17:58:06.772153 17664 misc.py:27] episode 73, reward -0.953134, avg reward -0.592796, total steps 9344, episode step 128\n",
      "[2020-04-23 17:58:06,772] episode 73, reward -0.953134, avg reward -0.592796, total steps 9344, episode step 128\n",
      "I0423 17:58:14.781131 17664 misc.py:27] episode 74, reward 0.702274, avg reward -0.575295, total steps 9472, episode step 128\n",
      "[2020-04-23 17:58:14,781] episode 74, reward 0.702274, avg reward -0.575295, total steps 9472, episode step 128\n",
      "I0423 17:58:22.771306 17664 misc.py:27] episode 75, reward -5.152481, avg reward -0.636324, total steps 9600, episode step 128\n",
      "[2020-04-23 17:58:22,771] episode 75, reward -5.152481, avg reward -0.636324, total steps 9600, episode step 128\n",
      "I0423 17:58:30.350375 17664 misc.py:27] episode 76, reward -2.055355, avg reward -0.654995, total steps 9728, episode step 128\n",
      "[2020-04-23 17:58:30,350] episode 76, reward -2.055355, avg reward -0.654995, total steps 9728, episode step 128\n",
      "I0423 17:58:38.669404 17664 misc.py:27] episode 77, reward 1.092864, avg reward -0.632296, total steps 9856, episode step 128\n",
      "[2020-04-23 17:58:38,669] episode 77, reward 1.092864, avg reward -0.632296, total steps 9856, episode step 128\n",
      "I0423 17:58:46.754976 17664 misc.py:27] episode 78, reward -0.203202, avg reward -0.626795, total steps 9984, episode step 128\n",
      "[2020-04-23 17:58:46,754] episode 78, reward -0.203202, avg reward -0.626795, total steps 9984, episode step 128\n",
      "I0423 17:58:55.783651 17664 misc.py:27] episode 79, reward -0.182114, avg reward -0.621166, total steps 10112, episode step 128\n",
      "[2020-04-23 17:58:55,783] episode 79, reward -0.182114, avg reward -0.621166, total steps 10112, episode step 128\n",
      "I0423 17:59:03.795737 17664 misc.py:27] episode 80, reward 5.300153, avg reward -0.547149, total steps 10240, episode step 128\n",
      "[2020-04-23 17:59:03,795] episode 80, reward 5.300153, avg reward -0.547149, total steps 10240, episode step 128\n",
      "I0423 17:59:03.802104 17664 misc.py:47] Testing...\n",
      "[2020-04-23 17:59:03,802] Testing...\n",
      "I0423 17:59:04.616323 17664 misc.py:55] Avg reward 7.709720(0.000000)\n",
      "[2020-04-23 17:59:04,616] Avg reward 7.709720(0.000000)\n",
      "I0423 17:59:12.993519 17664 misc.py:27] episode 81, reward 2.118766, avg reward -0.514237, total steps 10368, episode step 128\n",
      "[2020-04-23 17:59:12,993] episode 81, reward 2.118766, avg reward -0.514237, total steps 10368, episode step 128\n",
      "I0423 17:59:21.713353 17664 misc.py:27] episode 82, reward -3.175577, avg reward -0.546692, total steps 10496, episode step 128\n",
      "[2020-04-23 17:59:21,713] episode 82, reward -3.175577, avg reward -0.546692, total steps 10496, episode step 128\n",
      "I0423 17:59:30.368460 17664 misc.py:27] episode 83, reward 5.630894, avg reward -0.472263, total steps 10624, episode step 128\n",
      "[2020-04-23 17:59:30,368] episode 83, reward 5.630894, avg reward -0.472263, total steps 10624, episode step 128\n",
      "I0423 17:59:37.647372 17664 misc.py:27] episode 84, reward -1.390767, avg reward -0.483198, total steps 10752, episode step 128\n",
      "[2020-04-23 17:59:37,647] episode 84, reward -1.390767, avg reward -0.483198, total steps 10752, episode step 128\n",
      "I0423 17:59:44.588872 17664 misc.py:27] episode 85, reward -9.029445, avg reward -0.583742, total steps 10880, episode step 128\n",
      "[2020-04-23 17:59:44,588] episode 85, reward -9.029445, avg reward -0.583742, total steps 10880, episode step 128\n",
      "I0423 17:59:52.402580 17664 misc.py:27] episode 86, reward -10.436194, avg reward -0.698305, total steps 11008, episode step 128\n",
      "[2020-04-23 17:59:52,402] episode 86, reward -10.436194, avg reward -0.698305, total steps 11008, episode step 128\n",
      "I0423 18:00:00.152080 17664 misc.py:27] episode 87, reward -3.959638, avg reward -0.735792, total steps 11136, episode step 128\n",
      "[2020-04-23 18:00:00,152] episode 87, reward -3.959638, avg reward -0.735792, total steps 11136, episode step 128\n",
      "I0423 18:00:08.134431 17664 misc.py:27] episode 88, reward 3.422221, avg reward -0.688542, total steps 11264, episode step 128\n",
      "[2020-04-23 18:00:08,134] episode 88, reward 3.422221, avg reward -0.688542, total steps 11264, episode step 128\n",
      "I0423 18:00:17.448237 17664 misc.py:27] episode 89, reward -1.739411, avg reward -0.700349, total steps 11392, episode step 128\n",
      "[2020-04-23 18:00:17,448] episode 89, reward -1.739411, avg reward -0.700349, total steps 11392, episode step 128\n",
      "I0423 18:00:25.118026 17664 misc.py:27] episode 90, reward -3.181933, avg reward -0.727923, total steps 11520, episode step 128\n",
      "[2020-04-23 18:00:25,118] episode 90, reward -3.181933, avg reward -0.727923, total steps 11520, episode step 128\n",
      "I0423 18:00:25.124557 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:00:25,124] Testing...\n",
      "I0423 18:00:25.940816 17664 misc.py:55] Avg reward -3.155604(0.000000)\n",
      "[2020-04-23 18:00:25,940] Avg reward -3.155604(0.000000)\n",
      "I0423 18:00:33.219307 17664 misc.py:27] episode 91, reward 1.483136, avg reward -0.703625, total steps 11648, episode step 128\n",
      "[2020-04-23 18:00:33,219] episode 91, reward 1.483136, avg reward -0.703625, total steps 11648, episode step 128\n",
      "I0423 18:00:41.214835 17664 misc.py:27] episode 92, reward -0.073911, avg reward -0.696781, total steps 11776, episode step 128\n",
      "[2020-04-23 18:00:41,214] episode 92, reward -0.073911, avg reward -0.696781, total steps 11776, episode step 128\n",
      "I0423 18:00:49.225139 17664 misc.py:27] episode 93, reward 2.517938, avg reward -0.662214, total steps 11904, episode step 128\n",
      "[2020-04-23 18:00:49,225] episode 93, reward 2.517938, avg reward -0.662214, total steps 11904, episode step 128\n",
      "I0423 18:00:56.860809 17664 misc.py:27] episode 94, reward -1.553189, avg reward -0.671692, total steps 12032, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-23 18:00:56,860] episode 94, reward -1.553189, avg reward -0.671692, total steps 12032, episode step 128\n",
      "I0423 18:01:04.286479 17664 misc.py:27] episode 95, reward -4.874760, avg reward -0.715935, total steps 12160, episode step 128\n",
      "[2020-04-23 18:01:04,286] episode 95, reward -4.874760, avg reward -0.715935, total steps 12160, episode step 128\n",
      "I0423 18:01:11.006934 17664 misc.py:27] episode 96, reward -2.371484, avg reward -0.733180, total steps 12288, episode step 128\n",
      "[2020-04-23 18:01:11,006] episode 96, reward -2.371484, avg reward -0.733180, total steps 12288, episode step 128\n",
      "I0423 18:01:18.161355 17664 misc.py:27] episode 97, reward -1.172092, avg reward -0.737705, total steps 12416, episode step 128\n",
      "[2020-04-23 18:01:18,161] episode 97, reward -1.172092, avg reward -0.737705, total steps 12416, episode step 128\n",
      "I0423 18:01:24.597156 17664 misc.py:27] episode 98, reward -0.949174, avg reward -0.739863, total steps 12544, episode step 128\n",
      "[2020-04-23 18:01:24,597] episode 98, reward -0.949174, avg reward -0.739863, total steps 12544, episode step 128\n",
      "I0423 18:01:31.021211 17664 misc.py:27] episode 99, reward -6.013693, avg reward -0.793134, total steps 12672, episode step 128\n",
      "[2020-04-23 18:01:31,021] episode 99, reward -6.013693, avg reward -0.793134, total steps 12672, episode step 128\n",
      "I0423 18:01:38.830810 17664 misc.py:27] episode 100, reward -3.520094, avg reward -0.820404, total steps 12800, episode step 128\n",
      "[2020-04-23 18:01:38,830] episode 100, reward -3.520094, avg reward -0.820404, total steps 12800, episode step 128\n",
      "I0423 18:01:38.830810 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:01:38,830] Testing...\n",
      "I0423 18:01:39.698412 17664 misc.py:55] Avg reward 2.374407(0.000000)\n",
      "[2020-04-23 18:01:39,698] Avg reward 2.374407(0.000000)\n",
      "I0423 18:01:47.736429 17664 misc.py:27] episode 101, reward -6.550708, avg reward -0.855518, total steps 12928, episode step 128\n",
      "[2020-04-23 18:01:47,736] episode 101, reward -6.550708, avg reward -0.855518, total steps 12928, episode step 128\n",
      "I0423 18:01:54.011867 17664 misc.py:27] episode 102, reward -3.790213, avg reward -0.851396, total steps 13056, episode step 128\n",
      "[2020-04-23 18:01:54,011] episode 102, reward -3.790213, avg reward -0.851396, total steps 13056, episode step 128\n",
      "I0423 18:02:00.235618 17664 misc.py:27] episode 103, reward -0.864393, avg reward -0.815158, total steps 13184, episode step 128\n",
      "[2020-04-23 18:02:00,235] episode 103, reward -0.864393, avg reward -0.815158, total steps 13184, episode step 128\n",
      "I0423 18:02:08.125716 17664 misc.py:27] episode 104, reward 0.188231, avg reward -0.842383, total steps 13312, episode step 128\n",
      "[2020-04-23 18:02:08,125] episode 104, reward 0.188231, avg reward -0.842383, total steps 13312, episode step 128\n",
      "I0423 18:02:15.531920 17664 misc.py:27] episode 105, reward -8.093305, avg reward -0.906760, total steps 13440, episode step 128\n",
      "[2020-04-23 18:02:15,531] episode 105, reward -8.093305, avg reward -0.906760, total steps 13440, episode step 128\n",
      "I0423 18:02:23.507311 17664 misc.py:27] episode 106, reward 3.735458, avg reward -0.834838, total steps 13568, episode step 128\n",
      "[2020-04-23 18:02:23,507] episode 106, reward 3.735458, avg reward -0.834838, total steps 13568, episode step 128\n",
      "I0423 18:02:30.920972 17664 misc.py:27] episode 107, reward -7.687124, avg reward -0.892926, total steps 13696, episode step 128\n",
      "[2020-04-23 18:02:30,920] episode 107, reward -7.687124, avg reward -0.892926, total steps 13696, episode step 128\n",
      "I0423 18:02:38.650691 17664 misc.py:27] episode 108, reward -1.072282, avg reward -0.881623, total steps 13824, episode step 128\n",
      "[2020-04-23 18:02:38,650] episode 108, reward -1.072282, avg reward -0.881623, total steps 13824, episode step 128\n",
      "I0423 18:02:46.556144 17664 misc.py:27] episode 109, reward -10.919107, avg reward -0.985338, total steps 13952, episode step 128\n",
      "[2020-04-23 18:02:46,556] episode 109, reward -10.919107, avg reward -0.985338, total steps 13952, episode step 128\n",
      "I0423 18:02:54.293839 17664 misc.py:27] episode 110, reward -0.867168, avg reward -0.979185, total steps 14080, episode step 128\n",
      "[2020-04-23 18:02:54,293] episode 110, reward -0.867168, avg reward -0.979185, total steps 14080, episode step 128\n",
      "I0423 18:02:54.298524 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:02:54,298] Testing...\n",
      "I0423 18:02:55.015206 17664 misc.py:55] Avg reward -2.246275(0.000000)\n",
      "[2020-04-23 18:02:55,015] Avg reward -2.246275(0.000000)\n",
      "I0423 18:03:01.835500 17664 misc.py:27] episode 111, reward -3.047270, avg reward -0.990533, total steps 14208, episode step 128\n",
      "[2020-04-23 18:03:01,835] episode 111, reward -3.047270, avg reward -0.990533, total steps 14208, episode step 128\n",
      "I0423 18:03:09.289597 17664 misc.py:27] episode 112, reward -3.237381, avg reward -0.976007, total steps 14336, episode step 128\n",
      "[2020-04-23 18:03:09,289] episode 112, reward -3.237381, avg reward -0.976007, total steps 14336, episode step 128\n",
      "I0423 18:03:17.653712 17664 misc.py:27] episode 113, reward -7.292736, avg reward -1.037531, total steps 14464, episode step 128\n",
      "[2020-04-23 18:03:17,653] episode 113, reward -7.292736, avg reward -1.037531, total steps 14464, episode step 128\n",
      "I0423 18:03:26.518908 17664 misc.py:27] episode 114, reward 2.622322, avg reward -0.971157, total steps 14592, episode step 128\n",
      "[2020-04-23 18:03:26,518] episode 114, reward 2.622322, avg reward -0.971157, total steps 14592, episode step 128\n",
      "I0423 18:03:34.981545 17664 misc.py:27] episode 115, reward -1.840580, avg reward -0.977830, total steps 14720, episode step 128\n",
      "[2020-04-23 18:03:34,981] episode 115, reward -1.840580, avg reward -0.977830, total steps 14720, episode step 128\n",
      "I0423 18:03:43.302987 17664 misc.py:27] episode 116, reward -3.045757, avg reward -0.985681, total steps 14848, episode step 128\n",
      "[2020-04-23 18:03:43,302] episode 116, reward -3.045757, avg reward -0.985681, total steps 14848, episode step 128\n",
      "I0423 18:03:51.346798 17664 misc.py:27] episode 117, reward -1.336090, avg reward -1.010846, total steps 14976, episode step 128\n",
      "[2020-04-23 18:03:51,346] episode 117, reward -1.336090, avg reward -1.010846, total steps 14976, episode step 128\n",
      "I0423 18:03:58.443480 17664 misc.py:27] episode 118, reward -2.653720, avg reward -1.075981, total steps 15104, episode step 128\n",
      "[2020-04-23 18:03:58,443] episode 118, reward -2.653720, avg reward -1.075981, total steps 15104, episode step 128\n",
      "I0423 18:04:06.429277 17664 misc.py:27] episode 119, reward 0.212353, avg reward -1.031499, total steps 15232, episode step 128\n",
      "[2020-04-23 18:04:06,429] episode 119, reward 0.212353, avg reward -1.031499, total steps 15232, episode step 128\n",
      "I0423 18:04:13.623804 17664 misc.py:27] episode 120, reward -4.849124, avg reward -1.272099, total steps 15360, episode step 128\n",
      "[2020-04-23 18:04:13,623] episode 120, reward -4.849124, avg reward -1.272099, total steps 15360, episode step 128\n",
      "I0423 18:04:13.633805 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:04:13,633] Testing...\n",
      "I0423 18:04:14.440401 17664 misc.py:55] Avg reward -3.490710(0.000000)\n",
      "[2020-04-23 18:04:14,440] Avg reward -3.490710(0.000000)\n",
      "I0423 18:04:22.213041 17664 misc.py:27] episode 121, reward -3.146867, avg reward -1.348429, total steps 15488, episode step 128\n",
      "[2020-04-23 18:04:22,213] episode 121, reward -3.146867, avg reward -1.348429, total steps 15488, episode step 128\n",
      "I0423 18:04:31.149733 17664 misc.py:27] episode 122, reward -2.765316, avg reward -1.307655, total steps 15616, episode step 128\n",
      "[2020-04-23 18:04:31,149] episode 122, reward -2.765316, avg reward -1.307655, total steps 15616, episode step 128\n",
      "I0423 18:04:39.082364 17664 misc.py:27] episode 123, reward 1.354475, avg reward -1.350616, total steps 15744, episode step 128\n",
      "[2020-04-23 18:04:39,082] episode 123, reward 1.354475, avg reward -1.350616, total steps 15744, episode step 128\n",
      "I0423 18:04:47.195403 17664 misc.py:27] episode 124, reward -8.195825, avg reward -1.401822, total steps 15872, episode step 128\n",
      "[2020-04-23 18:04:47,195] episode 124, reward -8.195825, avg reward -1.401822, total steps 15872, episode step 128\n",
      "I0423 18:04:55.255651 17664 misc.py:27] episode 125, reward -2.057791, avg reward -1.418155, total steps 16000, episode step 128\n",
      "[2020-04-23 18:04:55,255] episode 125, reward -2.057791, avg reward -1.418155, total steps 16000, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 18:05:02.866876 17664 misc.py:27] episode 126, reward 3.366789, avg reward -1.320511, total steps 16128, episode step 128\n",
      "[2020-04-23 18:05:02,866] episode 126, reward 3.366789, avg reward -1.320511, total steps 16128, episode step 128\n",
      "I0423 18:05:10.741499 17664 misc.py:27] episode 127, reward -1.304927, avg reward -1.292536, total steps 16256, episode step 128\n",
      "[2020-04-23 18:05:10,741] episode 127, reward -1.304927, avg reward -1.292536, total steps 16256, episode step 128\n",
      "I0423 18:05:18.679628 17664 misc.py:27] episode 128, reward -8.832321, avg reward -1.383169, total steps 16384, episode step 128\n",
      "[2020-04-23 18:05:18,679] episode 128, reward -8.832321, avg reward -1.383169, total steps 16384, episode step 128\n",
      "I0423 18:05:26.773726 17664 misc.py:27] episode 129, reward 11.892437, avg reward -1.238300, total steps 16512, episode step 128\n",
      "[2020-04-23 18:05:26,773] episode 129, reward 11.892437, avg reward -1.238300, total steps 16512, episode step 128\n",
      "I0423 18:05:35.051793 17664 misc.py:27] episode 130, reward -0.714673, avg reward -1.217747, total steps 16640, episode step 128\n",
      "[2020-04-23 18:05:35,051] episode 130, reward -0.714673, avg reward -1.217747, total steps 16640, episode step 128\n",
      "I0423 18:05:35.061850 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:05:35,061] Testing...\n",
      "I0423 18:05:35.860694 17664 misc.py:55] Avg reward 7.715881(0.000000)\n",
      "[2020-04-23 18:05:35,860] Avg reward 7.715881(0.000000)\n",
      "I0423 18:05:43.345993 17664 misc.py:27] episode 131, reward -9.581912, avg reward -1.302151, total steps 16768, episode step 128\n",
      "[2020-04-23 18:05:43,345] episode 131, reward -9.581912, avg reward -1.302151, total steps 16768, episode step 128\n",
      "I0423 18:05:50.976867 17664 misc.py:27] episode 132, reward -3.258895, avg reward -1.324350, total steps 16896, episode step 128\n",
      "[2020-04-23 18:05:50,976] episode 132, reward -3.258895, avg reward -1.324350, total steps 16896, episode step 128\n",
      "I0423 18:05:58.086990 17664 misc.py:27] episode 133, reward -6.007431, avg reward -1.368296, total steps 17024, episode step 128\n",
      "[2020-04-23 18:05:58,086] episode 133, reward -6.007431, avg reward -1.368296, total steps 17024, episode step 128\n",
      "I0423 18:06:05.427922 17664 misc.py:27] episode 134, reward -6.370279, avg reward -1.495524, total steps 17152, episode step 128\n",
      "[2020-04-23 18:06:05,427] episode 134, reward -6.370279, avg reward -1.495524, total steps 17152, episode step 128\n",
      "I0423 18:06:12.642173 17664 misc.py:27] episode 135, reward -0.498384, avg reward -1.587020, total steps 17280, episode step 128\n",
      "[2020-04-23 18:06:12,642] episode 135, reward -0.498384, avg reward -1.587020, total steps 17280, episode step 128\n",
      "I0423 18:06:20.379208 17664 misc.py:27] episode 136, reward -6.968743, avg reward -1.647153, total steps 17408, episode step 128\n",
      "[2020-04-23 18:06:20,379] episode 136, reward -6.968743, avg reward -1.647153, total steps 17408, episode step 128\n",
      "I0423 18:06:27.532730 17664 misc.py:27] episode 137, reward 3.986681, avg reward -1.519662, total steps 17536, episode step 128\n",
      "[2020-04-23 18:06:27,532] episode 137, reward 3.986681, avg reward -1.519662, total steps 17536, episode step 128\n",
      "I0423 18:06:34.840751 17664 misc.py:27] episode 138, reward -6.448422, avg reward -1.526292, total steps 17664, episode step 128\n",
      "[2020-04-23 18:06:34,840] episode 138, reward -6.448422, avg reward -1.526292, total steps 17664, episode step 128\n",
      "I0423 18:06:42.522518 17664 misc.py:27] episode 139, reward -3.735228, avg reward -1.615960, total steps 17792, episode step 128\n",
      "[2020-04-23 18:06:42,522] episode 139, reward -3.735228, avg reward -1.615960, total steps 17792, episode step 128\n",
      "I0423 18:06:51.050056 17664 misc.py:27] episode 140, reward -1.627741, avg reward -1.757420, total steps 17920, episode step 128\n",
      "[2020-04-23 18:06:51,050] episode 140, reward -1.627741, avg reward -1.757420, total steps 17920, episode step 128\n",
      "I0423 18:06:51.058203 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:06:51,058] Testing...\n",
      "I0423 18:06:52.104441 17664 misc.py:55] Avg reward -3.377251(0.000000)\n",
      "[2020-04-23 18:06:52,104] Avg reward -3.377251(0.000000)\n",
      "I0423 18:07:00.032937 17664 misc.py:27] episode 141, reward 2.192067, avg reward -1.798434, total steps 18048, episode step 128\n",
      "[2020-04-23 18:07:00,032] episode 141, reward 2.192067, avg reward -1.798434, total steps 18048, episode step 128\n",
      "I0423 18:07:08.275455 17664 misc.py:27] episode 142, reward -1.129455, avg reward -1.816365, total steps 18176, episode step 128\n",
      "[2020-04-23 18:07:08,275] episode 142, reward -1.129455, avg reward -1.816365, total steps 18176, episode step 128\n",
      "I0423 18:07:15.578605 17664 misc.py:27] episode 143, reward 9.034811, avg reward -1.725307, total steps 18304, episode step 128\n",
      "[2020-04-23 18:07:15,578] episode 143, reward 9.034811, avg reward -1.725307, total steps 18304, episode step 128\n",
      "I0423 18:07:22.731112 17664 misc.py:27] episode 144, reward -6.123978, avg reward -1.709624, total steps 18432, episode step 128\n",
      "[2020-04-23 18:07:22,731] episode 144, reward -6.123978, avg reward -1.709624, total steps 18432, episode step 128\n",
      "I0423 18:07:29.870535 17664 misc.py:27] episode 145, reward -3.855806, avg reward -1.824443, total steps 18560, episode step 128\n",
      "[2020-04-23 18:07:29,870] episode 145, reward -3.855806, avg reward -1.824443, total steps 18560, episode step 128\n",
      "I0423 18:07:36.778880 17664 misc.py:27] episode 146, reward -6.130941, avg reward -1.860047, total steps 18688, episode step 128\n",
      "[2020-04-23 18:07:36,778] episode 146, reward -6.130941, avg reward -1.860047, total steps 18688, episode step 128\n",
      "I0423 18:07:44.185299 17664 misc.py:27] episode 147, reward -0.585190, avg reward -1.809500, total steps 18816, episode step 128\n",
      "[2020-04-23 18:07:44,185] episode 147, reward -0.585190, avg reward -1.809500, total steps 18816, episode step 128\n",
      "I0423 18:07:51.316175 17664 misc.py:27] episode 148, reward 9.304411, avg reward -1.721929, total steps 18944, episode step 128\n",
      "[2020-04-23 18:07:51,316] episode 148, reward 9.304411, avg reward -1.721929, total steps 18944, episode step 128\n",
      "I0423 18:07:58.678373 17664 misc.py:27] episode 149, reward -4.356212, avg reward -1.741210, total steps 19072, episode step 128\n",
      "[2020-04-23 18:07:58,678] episode 149, reward -4.356212, avg reward -1.741210, total steps 19072, episode step 128\n",
      "I0423 18:08:05.758929 17664 misc.py:27] episode 150, reward -6.656146, avg reward -1.803898, total steps 19200, episode step 128\n",
      "[2020-04-23 18:08:05,758] episode 150, reward -6.656146, avg reward -1.803898, total steps 19200, episode step 128\n",
      "I0423 18:08:05.762199 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:08:05,762] Testing...\n",
      "I0423 18:08:06.505649 17664 misc.py:55] Avg reward -2.133446(0.000000)\n",
      "[2020-04-23 18:08:06,505] Avg reward -2.133446(0.000000)\n",
      "I0423 18:08:13.812754 17664 misc.py:27] episode 151, reward -8.886678, avg reward -1.847283, total steps 19328, episode step 128\n",
      "[2020-04-23 18:08:13,812] episode 151, reward -8.886678, avg reward -1.847283, total steps 19328, episode step 128\n",
      "I0423 18:08:20.983931 17664 misc.py:27] episode 152, reward 5.048816, avg reward -1.767223, total steps 19456, episode step 128\n",
      "[2020-04-23 18:08:20,983] episode 152, reward 5.048816, avg reward -1.767223, total steps 19456, episode step 128\n",
      "I0423 18:08:28.538100 17664 misc.py:27] episode 153, reward -3.343671, avg reward -1.789472, total steps 19584, episode step 128\n",
      "[2020-04-23 18:08:28,538] episode 153, reward -3.343671, avg reward -1.789472, total steps 19584, episode step 128\n",
      "I0423 18:08:35.816581 17664 misc.py:27] episode 154, reward -0.262694, avg reward -1.749691, total steps 19712, episode step 128\n",
      "[2020-04-23 18:08:35,816] episode 154, reward -0.262694, avg reward -1.749691, total steps 19712, episode step 128\n",
      "I0423 18:08:43.901463 17664 misc.py:27] episode 155, reward -11.028526, avg reward -1.860812, total steps 19840, episode step 128\n",
      "[2020-04-23 18:08:43,901] episode 155, reward -11.028526, avg reward -1.860812, total steps 19840, episode step 128\n",
      "I0423 18:08:51.918968 17664 misc.py:27] episode 156, reward -5.758287, avg reward -1.838174, total steps 19968, episode step 128\n",
      "[2020-04-23 18:08:51,918] episode 156, reward -5.758287, avg reward -1.838174, total steps 19968, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 18:08:59.323894 17664 misc.py:27] episode 157, reward -3.648484, avg reward -1.867772, total steps 20096, episode step 128\n",
      "[2020-04-23 18:08:59,323] episode 157, reward -3.648484, avg reward -1.867772, total steps 20096, episode step 128\n",
      "I0423 18:09:07.148843 17664 misc.py:27] episode 158, reward -2.311639, avg reward -1.863101, total steps 20224, episode step 128\n",
      "[2020-04-23 18:09:07,148] episode 158, reward -2.311639, avg reward -1.863101, total steps 20224, episode step 128\n",
      "I0423 18:09:16.414099 17664 misc.py:27] episode 159, reward -2.122421, avg reward -1.947701, total steps 20352, episode step 128\n",
      "[2020-04-23 18:09:16,414] episode 159, reward -2.122421, avg reward -1.947701, total steps 20352, episode step 128\n",
      "I0423 18:09:24.004026 17664 misc.py:27] episode 160, reward -4.416247, avg reward -2.047596, total steps 20480, episode step 128\n",
      "[2020-04-23 18:09:24,004] episode 160, reward -4.416247, avg reward -2.047596, total steps 20480, episode step 128\n",
      "I0423 18:09:24.012212 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:09:24,012] Testing...\n",
      "I0423 18:09:24.860362 17664 misc.py:55] Avg reward -4.140998(0.000000)\n",
      "[2020-04-23 18:09:24,860] Avg reward -4.140998(0.000000)\n",
      "I0423 18:09:32.396320 17664 misc.py:27] episode 161, reward 2.480006, avg reward -2.000685, total steps 20608, episode step 128\n",
      "[2020-04-23 18:09:32,396] episode 161, reward 2.480006, avg reward -2.000685, total steps 20608, episode step 128\n",
      "I0423 18:09:40.048636 17664 misc.py:27] episode 162, reward -3.716216, avg reward -2.107905, total steps 20736, episode step 128\n",
      "[2020-04-23 18:09:40,048] episode 162, reward -3.716216, avg reward -2.107905, total steps 20736, episode step 128\n",
      "I0423 18:09:48.205586 17664 misc.py:27] episode 163, reward -1.349650, avg reward -2.069096, total steps 20864, episode step 128\n",
      "[2020-04-23 18:09:48,205] episode 163, reward -1.349650, avg reward -2.069096, total steps 20864, episode step 128\n",
      "I0423 18:09:56.320740 17664 misc.py:27] episode 164, reward -4.151384, avg reward -2.105482, total steps 20992, episode step 128\n",
      "[2020-04-23 18:09:56,320] episode 164, reward -4.151384, avg reward -2.105482, total steps 20992, episode step 128\n",
      "I0423 18:10:04.101022 17664 misc.py:27] episode 165, reward -7.406720, avg reward -2.150836, total steps 21120, episode step 128\n",
      "[2020-04-23 18:10:04,101] episode 165, reward -7.406720, avg reward -2.150836, total steps 21120, episode step 128\n",
      "I0423 18:10:12.102524 17664 misc.py:27] episode 166, reward -3.039922, avg reward -2.178370, total steps 21248, episode step 128\n",
      "[2020-04-23 18:10:12,102] episode 166, reward -3.039922, avg reward -2.178370, total steps 21248, episode step 128\n",
      "I0423 18:10:19.775847 17664 misc.py:27] episode 167, reward -2.829181, avg reward -2.188155, total steps 21376, episode step 128\n",
      "[2020-04-23 18:10:19,775] episode 167, reward -2.829181, avg reward -2.188155, total steps 21376, episode step 128\n",
      "I0423 18:10:27.593640 17664 misc.py:27] episode 168, reward -3.265252, avg reward -2.262700, total steps 21504, episode step 128\n",
      "[2020-04-23 18:10:27,593] episode 168, reward -3.265252, avg reward -2.262700, total steps 21504, episode step 128\n",
      "I0423 18:10:35.673815 17664 misc.py:27] episode 169, reward -4.104459, avg reward -2.328420, total steps 21632, episode step 128\n",
      "[2020-04-23 18:10:35,673] episode 169, reward -4.104459, avg reward -2.328420, total steps 21632, episode step 128\n",
      "I0423 18:10:43.524914 17664 misc.py:27] episode 170, reward -7.197509, avg reward -2.425827, total steps 21760, episode step 128\n",
      "[2020-04-23 18:10:43,524] episode 170, reward -7.197509, avg reward -2.425827, total steps 21760, episode step 128\n",
      "I0423 18:10:43.532905 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:10:43,532] Testing...\n",
      "I0423 18:10:44.280035 17664 misc.py:55] Avg reward -0.972494(0.000000)\n",
      "[2020-04-23 18:10:44,280] Avg reward -0.972494(0.000000)\n",
      "I0423 18:10:52.070779 17664 misc.py:27] episode 171, reward -2.416946, avg reward -2.389334, total steps 21888, episode step 128\n",
      "[2020-04-23 18:10:52,070] episode 171, reward -2.416946, avg reward -2.389334, total steps 21888, episode step 128\n",
      "I0423 18:10:59.918391 17664 misc.py:27] episode 172, reward 5.076049, avg reward -2.279798, total steps 22016, episode step 128\n",
      "[2020-04-23 18:10:59,918] episode 172, reward 5.076049, avg reward -2.279798, total steps 22016, episode step 128\n",
      "I0423 18:11:08.527953 17664 misc.py:27] episode 173, reward -1.072156, avg reward -2.280988, total steps 22144, episode step 128\n",
      "[2020-04-23 18:11:08,527] episode 173, reward -1.072156, avg reward -2.280988, total steps 22144, episode step 128\n",
      "I0423 18:11:16.554039 17664 misc.py:27] episode 174, reward -10.715135, avg reward -2.395163, total steps 22272, episode step 128\n",
      "[2020-04-23 18:11:16,554] episode 174, reward -10.715135, avg reward -2.395163, total steps 22272, episode step 128\n",
      "I0423 18:11:24.755496 17664 misc.py:27] episode 175, reward -2.119765, avg reward -2.364835, total steps 22400, episode step 128\n",
      "[2020-04-23 18:11:24,755] episode 175, reward -2.119765, avg reward -2.364835, total steps 22400, episode step 128\n",
      "I0423 18:11:32.594575 17664 misc.py:27] episode 176, reward -5.111246, avg reward -2.395394, total steps 22528, episode step 128\n",
      "[2020-04-23 18:11:32,594] episode 176, reward -5.111246, avg reward -2.395394, total steps 22528, episode step 128\n",
      "I0423 18:11:40.674765 17664 misc.py:27] episode 177, reward -8.611382, avg reward -2.492437, total steps 22656, episode step 128\n",
      "[2020-04-23 18:11:40,674] episode 177, reward -8.611382, avg reward -2.492437, total steps 22656, episode step 128\n",
      "I0423 18:11:48.942145 17664 misc.py:27] episode 178, reward -5.126839, avg reward -2.541673, total steps 22784, episode step 128\n",
      "[2020-04-23 18:11:48,942] episode 178, reward -5.126839, avg reward -2.541673, total steps 22784, episode step 128\n",
      "I0423 18:11:56.899236 17664 misc.py:27] episode 179, reward -1.980300, avg reward -2.559655, total steps 22912, episode step 128\n",
      "[2020-04-23 18:11:56,899] episode 179, reward -1.980300, avg reward -2.559655, total steps 22912, episode step 128\n",
      "I0423 18:12:04.918799 17664 misc.py:27] episode 180, reward 3.002982, avg reward -2.582627, total steps 23040, episode step 128\n",
      "[2020-04-23 18:12:04,918] episode 180, reward 3.002982, avg reward -2.582627, total steps 23040, episode step 128\n",
      "I0423 18:12:04.918799 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:12:04,918] Testing...\n",
      "I0423 18:12:05.737425 17664 misc.py:55] Avg reward -1.742836(0.000000)\n",
      "[2020-04-23 18:12:05,737] Avg reward -1.742836(0.000000)\n",
      "I0423 18:12:13.401849 17664 misc.py:27] episode 181, reward -4.774252, avg reward -2.651557, total steps 23168, episode step 128\n",
      "[2020-04-23 18:12:13,401] episode 181, reward -4.774252, avg reward -2.651557, total steps 23168, episode step 128\n",
      "I0423 18:12:20.741552 17664 misc.py:27] episode 182, reward 9.047236, avg reward -2.529329, total steps 23296, episode step 128\n",
      "[2020-04-23 18:12:20,741] episode 182, reward 9.047236, avg reward -2.529329, total steps 23296, episode step 128\n",
      "I0423 18:12:28.372324 17664 misc.py:27] episode 183, reward -2.764986, avg reward -2.613287, total steps 23424, episode step 128\n",
      "[2020-04-23 18:12:28,372] episode 183, reward -2.764986, avg reward -2.613287, total steps 23424, episode step 128\n",
      "I0423 18:12:36.228607 17664 misc.py:27] episode 184, reward -7.469145, avg reward -2.674071, total steps 23552, episode step 128\n",
      "[2020-04-23 18:12:36,228] episode 184, reward -7.469145, avg reward -2.674071, total steps 23552, episode step 128\n",
      "I0423 18:12:44.723137 17664 misc.py:27] episode 185, reward 5.373740, avg reward -2.530039, total steps 23680, episode step 128\n",
      "[2020-04-23 18:12:44,723] episode 185, reward 5.373740, avg reward -2.530039, total steps 23680, episode step 128\n",
      "I0423 18:12:53.326387 17664 misc.py:27] episode 186, reward -2.307247, avg reward -2.448750, total steps 23808, episode step 128\n",
      "[2020-04-23 18:12:53,326] episode 186, reward -2.307247, avg reward -2.448750, total steps 23808, episode step 128\n",
      "I0423 18:13:01.845063 17664 misc.py:27] episode 187, reward 3.916268, avg reward -2.369991, total steps 23936, episode step 128\n",
      "[2020-04-23 18:13:01,845] episode 187, reward 3.916268, avg reward -2.369991, total steps 23936, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 18:13:10.559180 17664 misc.py:27] episode 188, reward -2.824945, avg reward -2.432463, total steps 24064, episode step 128\n",
      "[2020-04-23 18:13:10,559] episode 188, reward -2.824945, avg reward -2.432463, total steps 24064, episode step 128\n",
      "I0423 18:13:18.842967 17664 misc.py:27] episode 189, reward 4.621853, avg reward -2.368850, total steps 24192, episode step 128\n",
      "[2020-04-23 18:13:18,842] episode 189, reward 4.621853, avg reward -2.368850, total steps 24192, episode step 128\n",
      "I0423 18:13:26.594666 17664 misc.py:27] episode 190, reward 3.797255, avg reward -2.299058, total steps 24320, episode step 128\n",
      "[2020-04-23 18:13:26,594] episode 190, reward 3.797255, avg reward -2.299058, total steps 24320, episode step 128\n",
      "I0423 18:13:26.598955 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:13:26,598] Testing...\n",
      "I0423 18:13:27.415964 17664 misc.py:55] Avg reward -4.458004(0.000000)\n",
      "[2020-04-23 18:13:27,415] Avg reward -4.458004(0.000000)\n",
      "I0423 18:13:35.912255 17664 misc.py:27] episode 191, reward 0.690837, avg reward -2.306981, total steps 24448, episode step 128\n",
      "[2020-04-23 18:13:35,912] episode 191, reward 0.690837, avg reward -2.306981, total steps 24448, episode step 128\n",
      "I0423 18:13:44.591592 17664 misc.py:27] episode 192, reward -1.471650, avg reward -2.320958, total steps 24576, episode step 128\n",
      "[2020-04-23 18:13:44,591] episode 192, reward -1.471650, avg reward -2.320958, total steps 24576, episode step 128\n",
      "I0423 18:13:53.736866 17664 misc.py:27] episode 193, reward -5.803779, avg reward -2.404176, total steps 24704, episode step 128\n",
      "[2020-04-23 18:13:53,736] episode 193, reward -5.803779, avg reward -2.404176, total steps 24704, episode step 128\n",
      "I0423 18:14:02.404602 17664 misc.py:27] episode 194, reward -5.956046, avg reward -2.448204, total steps 24832, episode step 128\n",
      "[2020-04-23 18:14:02,404] episode 194, reward -5.956046, avg reward -2.448204, total steps 24832, episode step 128\n",
      "I0423 18:14:11.351375 17664 misc.py:27] episode 195, reward 0.212701, avg reward -2.397330, total steps 24960, episode step 128\n",
      "[2020-04-23 18:14:11,351] episode 195, reward 0.212701, avg reward -2.397330, total steps 24960, episode step 128\n",
      "I0423 18:14:20.098522 17664 misc.py:27] episode 196, reward -6.472070, avg reward -2.438335, total steps 25088, episode step 128\n",
      "[2020-04-23 18:14:20,098] episode 196, reward -6.472070, avg reward -2.438335, total steps 25088, episode step 128\n",
      "I0423 18:14:28.479475 17664 misc.py:27] episode 197, reward 1.399959, avg reward -2.412615, total steps 25216, episode step 128\n",
      "[2020-04-23 18:14:28,479] episode 197, reward 1.399959, avg reward -2.412615, total steps 25216, episode step 128\n",
      "I0423 18:14:37.263788 17664 misc.py:27] episode 198, reward -3.154590, avg reward -2.434669, total steps 25344, episode step 128\n",
      "[2020-04-23 18:14:37,263] episode 198, reward -3.154590, avg reward -2.434669, total steps 25344, episode step 128\n",
      "I0423 18:14:45.755296 17664 misc.py:27] episode 199, reward 22.241322, avg reward -2.152119, total steps 25472, episode step 128\n",
      "[2020-04-23 18:14:45,755] episode 199, reward 22.241322, avg reward -2.152119, total steps 25472, episode step 128\n",
      "I0423 18:14:54.381412 17664 misc.py:27] episode 200, reward -7.316916, avg reward -2.190087, total steps 25600, episode step 128\n",
      "[2020-04-23 18:14:54,381] episode 200, reward -7.316916, avg reward -2.190087, total steps 25600, episode step 128\n",
      "I0423 18:14:54.393223 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:14:54,393] Testing...\n",
      "I0423 18:14:55.087582 17664 misc.py:55] Avg reward -6.129155(0.000000)\n",
      "[2020-04-23 18:14:55,087] Avg reward -6.129155(0.000000)\n",
      "I0423 18:15:03.352026 17664 misc.py:27] episode 201, reward 0.138139, avg reward -2.123199, total steps 25728, episode step 128\n",
      "[2020-04-23 18:15:03,352] episode 201, reward 0.138139, avg reward -2.123199, total steps 25728, episode step 128\n",
      "I0423 18:15:12.028692 17664 misc.py:27] episode 202, reward -5.628114, avg reward -2.141578, total steps 25856, episode step 128\n",
      "[2020-04-23 18:15:12,028] episode 202, reward -5.628114, avg reward -2.141578, total steps 25856, episode step 128\n",
      "I0423 18:15:22.276025 17664 misc.py:27] episode 203, reward -2.542599, avg reward -2.158360, total steps 25984, episode step 128\n",
      "[2020-04-23 18:15:22,276] episode 203, reward -2.542599, avg reward -2.158360, total steps 25984, episode step 128\n",
      "I0423 18:15:30.711664 17664 misc.py:27] episode 204, reward 2.525814, avg reward -2.134984, total steps 26112, episode step 128\n",
      "[2020-04-23 18:15:30,711] episode 204, reward 2.525814, avg reward -2.134984, total steps 26112, episode step 128\n",
      "I0423 18:15:39.352683 17664 misc.py:27] episode 205, reward -2.739534, avg reward -2.081446, total steps 26240, episode step 128\n",
      "[2020-04-23 18:15:39,352] episode 205, reward -2.739534, avg reward -2.081446, total steps 26240, episode step 128\n",
      "I0423 18:15:47.984683 17664 misc.py:27] episode 206, reward -1.565282, avg reward -2.134454, total steps 26368, episode step 128\n",
      "[2020-04-23 18:15:47,984] episode 206, reward -1.565282, avg reward -2.134454, total steps 26368, episode step 128\n",
      "I0423 18:15:56.595534 17664 misc.py:27] episode 207, reward -5.350842, avg reward -2.111091, total steps 26496, episode step 128\n",
      "[2020-04-23 18:15:56,595] episode 207, reward -5.350842, avg reward -2.111091, total steps 26496, episode step 128\n",
      "I0423 18:16:05.009061 17664 misc.py:27] episode 208, reward -7.849984, avg reward -2.178868, total steps 26624, episode step 128\n",
      "[2020-04-23 18:16:05,009] episode 208, reward -7.849984, avg reward -2.178868, total steps 26624, episode step 128\n",
      "I0423 18:16:13.609736 17664 misc.py:27] episode 209, reward -2.278892, avg reward -2.092466, total steps 26752, episode step 128\n",
      "[2020-04-23 18:16:13,609] episode 209, reward -2.278892, avg reward -2.092466, total steps 26752, episode step 128\n",
      "I0423 18:16:22.199468 17664 misc.py:27] episode 210, reward -1.233281, avg reward -2.096127, total steps 26880, episode step 128\n",
      "[2020-04-23 18:16:22,199] episode 210, reward -1.233281, avg reward -2.096127, total steps 26880, episode step 128\n",
      "I0423 18:16:22.204594 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:16:22,204] Testing...\n",
      "I0423 18:16:22.949271 17664 misc.py:55] Avg reward 3.631888(0.000000)\n",
      "[2020-04-23 18:16:22,949] Avg reward 3.631888(0.000000)\n",
      "I0423 18:16:31.738214 17664 misc.py:27] episode 211, reward -5.054507, avg reward -2.116199, total steps 27008, episode step 128\n",
      "[2020-04-23 18:16:31,738] episode 211, reward -5.054507, avg reward -2.116199, total steps 27008, episode step 128\n",
      "I0423 18:16:40.165017 17664 misc.py:27] episode 212, reward -5.803340, avg reward -2.141859, total steps 27136, episode step 128\n",
      "[2020-04-23 18:16:40,165] episode 212, reward -5.803340, avg reward -2.141859, total steps 27136, episode step 128\n",
      "I0423 18:16:48.936165 17664 misc.py:27] episode 213, reward -0.260469, avg reward -2.071536, total steps 27264, episode step 128\n",
      "[2020-04-23 18:16:48,936] episode 213, reward -0.260469, avg reward -2.071536, total steps 27264, episode step 128\n",
      "I0423 18:16:57.134110 17664 misc.py:27] episode 214, reward -10.753499, avg reward -2.205294, total steps 27392, episode step 128\n",
      "[2020-04-23 18:16:57,134] episode 214, reward -10.753499, avg reward -2.205294, total steps 27392, episode step 128\n",
      "I0423 18:17:05.776524 17664 misc.py:27] episode 215, reward 3.979871, avg reward -2.147090, total steps 27520, episode step 128\n",
      "[2020-04-23 18:17:05,776] episode 215, reward 3.979871, avg reward -2.147090, total steps 27520, episode step 128\n",
      "I0423 18:17:14.013698 17664 misc.py:27] episode 216, reward -3.450486, avg reward -2.151137, total steps 27648, episode step 128\n",
      "[2020-04-23 18:17:14,013] episode 216, reward -3.450486, avg reward -2.151137, total steps 27648, episode step 128\n",
      "I0423 18:17:22.586519 17664 misc.py:27] episode 217, reward 0.551380, avg reward -2.132262, total steps 27776, episode step 128\n",
      "[2020-04-23 18:17:22,586] episode 217, reward 0.551380, avg reward -2.132262, total steps 27776, episode step 128\n",
      "I0423 18:17:31.160741 17664 misc.py:27] episode 218, reward -7.103012, avg reward -2.176755, total steps 27904, episode step 128\n",
      "[2020-04-23 18:17:31,160] episode 218, reward -7.103012, avg reward -2.176755, total steps 27904, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 18:17:39.233002 17664 misc.py:27] episode 219, reward -2.432594, avg reward -2.203205, total steps 28032, episode step 128\n",
      "[2020-04-23 18:17:39,233] episode 219, reward -2.432594, avg reward -2.203205, total steps 28032, episode step 128\n",
      "I0423 18:17:47.427274 17664 misc.py:27] episode 220, reward 1.713356, avg reward -2.137580, total steps 28160, episode step 128\n",
      "[2020-04-23 18:17:47,427] episode 220, reward 1.713356, avg reward -2.137580, total steps 28160, episode step 128\n",
      "I0423 18:17:47.433345 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:17:47,433] Testing...\n",
      "I0423 18:17:48.564819 17664 misc.py:55] Avg reward 0.757251(0.000000)\n",
      "[2020-04-23 18:17:48,564] Avg reward 0.757251(0.000000)\n",
      "I0423 18:17:56.784764 17664 misc.py:27] episode 221, reward -3.146776, avg reward -2.137579, total steps 28288, episode step 128\n",
      "[2020-04-23 18:17:56,784] episode 221, reward -3.146776, avg reward -2.137579, total steps 28288, episode step 128\n",
      "I0423 18:18:05.334203 17664 misc.py:27] episode 222, reward -7.017087, avg reward -2.180097, total steps 28416, episode step 128\n",
      "[2020-04-23 18:18:05,334] episode 222, reward -7.017087, avg reward -2.180097, total steps 28416, episode step 128\n",
      "I0423 18:18:13.747164 17664 misc.py:27] episode 223, reward -4.745449, avg reward -2.241096, total steps 28544, episode step 128\n",
      "[2020-04-23 18:18:13,747] episode 223, reward -4.745449, avg reward -2.241096, total steps 28544, episode step 128\n",
      "I0423 18:18:22.580816 17664 misc.py:27] episode 224, reward 10.117150, avg reward -2.057966, total steps 28672, episode step 128\n",
      "[2020-04-23 18:18:22,580] episode 224, reward 10.117150, avg reward -2.057966, total steps 28672, episode step 128\n",
      "I0423 18:18:30.907840 17664 misc.py:27] episode 225, reward -5.218370, avg reward -2.089572, total steps 28800, episode step 128\n",
      "[2020-04-23 18:18:30,907] episode 225, reward -5.218370, avg reward -2.089572, total steps 28800, episode step 128\n",
      "I0423 18:18:39.291119 17664 misc.py:27] episode 226, reward -1.606442, avg reward -2.139304, total steps 28928, episode step 128\n",
      "[2020-04-23 18:18:39,291] episode 226, reward -1.606442, avg reward -2.139304, total steps 28928, episode step 128\n",
      "I0423 18:18:47.009049 17664 misc.py:27] episode 227, reward -5.380541, avg reward -2.180060, total steps 29056, episode step 128\n",
      "[2020-04-23 18:18:47,009] episode 227, reward -5.380541, avg reward -2.180060, total steps 29056, episode step 128\n",
      "I0423 18:18:54.743541 17664 misc.py:27] episode 228, reward -5.837676, avg reward -2.150114, total steps 29184, episode step 128\n",
      "[2020-04-23 18:18:54,743] episode 228, reward -5.837676, avg reward -2.150114, total steps 29184, episode step 128\n",
      "I0423 18:19:02.688332 17664 misc.py:27] episode 229, reward -6.002975, avg reward -2.329068, total steps 29312, episode step 128\n",
      "[2020-04-23 18:19:02,688] episode 229, reward -6.002975, avg reward -2.329068, total steps 29312, episode step 128\n",
      "I0423 18:19:10.474205 17664 misc.py:27] episode 230, reward -8.412395, avg reward -2.406045, total steps 29440, episode step 128\n",
      "[2020-04-23 18:19:10,474] episode 230, reward -8.412395, avg reward -2.406045, total steps 29440, episode step 128\n",
      "I0423 18:19:10.482194 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:19:10,482] Testing...\n",
      "I0423 18:19:11.254264 17664 misc.py:55] Avg reward -2.760250(0.000000)\n",
      "[2020-04-23 18:19:11,254] Avg reward -2.760250(0.000000)\n",
      "I0423 18:19:19.194338 17664 misc.py:27] episode 231, reward 1.784768, avg reward -2.292379, total steps 29568, episode step 128\n",
      "[2020-04-23 18:19:19,194] episode 231, reward 1.784768, avg reward -2.292379, total steps 29568, episode step 128\n",
      "I0423 18:19:27.210005 17664 misc.py:27] episode 232, reward -3.637972, avg reward -2.296169, total steps 29696, episode step 128\n",
      "[2020-04-23 18:19:27,210] episode 232, reward -3.637972, avg reward -2.296169, total steps 29696, episode step 128\n",
      "I0423 18:19:34.796005 17664 misc.py:27] episode 233, reward -2.836562, avg reward -2.264461, total steps 29824, episode step 128\n",
      "[2020-04-23 18:19:34,796] episode 233, reward -2.836562, avg reward -2.264461, total steps 29824, episode step 128\n",
      "I0423 18:19:42.377657 17664 misc.py:27] episode 234, reward 2.558913, avg reward -2.175169, total steps 29952, episode step 128\n",
      "[2020-04-23 18:19:42,377] episode 234, reward 2.558913, avg reward -2.175169, total steps 29952, episode step 128\n",
      "I0423 18:19:50.128407 17664 misc.py:27] episode 235, reward -5.102459, avg reward -2.221209, total steps 30080, episode step 128\n",
      "[2020-04-23 18:19:50,128] episode 235, reward -5.102459, avg reward -2.221209, total steps 30080, episode step 128\n",
      "I0423 18:19:57.722647 17664 misc.py:27] episode 236, reward -8.777534, avg reward -2.239297, total steps 30208, episode step 128\n",
      "[2020-04-23 18:19:57,722] episode 236, reward -8.777534, avg reward -2.239297, total steps 30208, episode step 128\n",
      "I0423 18:20:05.215283 17664 misc.py:27] episode 237, reward -1.049427, avg reward -2.289658, total steps 30336, episode step 128\n",
      "[2020-04-23 18:20:05,215] episode 237, reward -1.049427, avg reward -2.289658, total steps 30336, episode step 128\n",
      "I0423 18:20:13.009365 17664 misc.py:27] episode 238, reward -2.369924, avg reward -2.248873, total steps 30464, episode step 128\n",
      "[2020-04-23 18:20:13,009] episode 238, reward -2.369924, avg reward -2.248873, total steps 30464, episode step 128\n",
      "I0423 18:20:20.111158 17664 misc.py:27] episode 239, reward -1.002830, avg reward -2.221550, total steps 30592, episode step 128\n",
      "[2020-04-23 18:20:20,111] episode 239, reward -1.002830, avg reward -2.221550, total steps 30592, episode step 128\n",
      "I0423 18:20:27.724582 17664 misc.py:27] episode 240, reward -6.902242, avg reward -2.274295, total steps 30720, episode step 128\n",
      "[2020-04-23 18:20:27,724] episode 240, reward -6.902242, avg reward -2.274295, total steps 30720, episode step 128\n",
      "I0423 18:20:27.734179 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:20:27,734] Testing...\n",
      "I0423 18:20:28.584281 17664 misc.py:55] Avg reward -3.173946(0.000000)\n",
      "[2020-04-23 18:20:28,584] Avg reward -3.173946(0.000000)\n",
      "I0423 18:20:36.010657 17664 misc.py:27] episode 241, reward -5.606840, avg reward -2.352284, total steps 30848, episode step 128\n",
      "[2020-04-23 18:20:36,010] episode 241, reward -5.606840, avg reward -2.352284, total steps 30848, episode step 128\n",
      "I0423 18:20:43.490428 17664 misc.py:27] episode 242, reward -1.893374, avg reward -2.359923, total steps 30976, episode step 128\n",
      "[2020-04-23 18:20:43,490] episode 242, reward -1.893374, avg reward -2.359923, total steps 30976, episode step 128\n",
      "I0423 18:20:51.005081 17664 misc.py:27] episode 243, reward -0.677682, avg reward -2.457048, total steps 31104, episode step 128\n",
      "[2020-04-23 18:20:51,005] episode 243, reward -0.677682, avg reward -2.457048, total steps 31104, episode step 128\n",
      "I0423 18:20:58.102498 17664 misc.py:27] episode 244, reward -1.862332, avg reward -2.414431, total steps 31232, episode step 128\n",
      "[2020-04-23 18:20:58,102] episode 244, reward -1.862332, avg reward -2.414431, total steps 31232, episode step 128\n",
      "I0423 18:21:05.811938 17664 misc.py:27] episode 245, reward -5.240540, avg reward -2.428279, total steps 31360, episode step 128\n",
      "[2020-04-23 18:21:05,811] episode 245, reward -5.240540, avg reward -2.428279, total steps 31360, episode step 128\n",
      "I0423 18:21:13.108610 17664 misc.py:27] episode 246, reward -5.880431, avg reward -2.425773, total steps 31488, episode step 128\n",
      "[2020-04-23 18:21:13,108] episode 246, reward -5.880431, avg reward -2.425773, total steps 31488, episode step 128\n",
      "I0423 18:21:22.095271 17664 misc.py:27] episode 247, reward -2.763909, avg reward -2.447561, total steps 31616, episode step 128\n",
      "[2020-04-23 18:21:22,095] episode 247, reward -2.763909, avg reward -2.447561, total steps 31616, episode step 128\n",
      "I0423 18:21:28.911737 17664 misc.py:27] episode 248, reward -0.240822, avg reward -2.543013, total steps 31744, episode step 128\n",
      "[2020-04-23 18:21:28,911] episode 248, reward -0.240822, avg reward -2.543013, total steps 31744, episode step 128\n",
      "I0423 18:21:35.510556 17664 misc.py:27] episode 249, reward 5.036243, avg reward -2.449088, total steps 31872, episode step 128\n",
      "[2020-04-23 18:21:35,510] episode 249, reward 5.036243, avg reward -2.449088, total steps 31872, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 18:21:43.025469 17664 misc.py:27] episode 250, reward -13.712670, avg reward -2.519654, total steps 32000, episode step 128\n",
      "[2020-04-23 18:21:43,025] episode 250, reward -13.712670, avg reward -2.519654, total steps 32000, episode step 128\n",
      "I0423 18:21:43.028793 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:21:43,028] Testing...\n",
      "I0423 18:21:43.719505 17664 misc.py:55] Avg reward -11.325607(0.000000)\n",
      "[2020-04-23 18:21:43,719] Avg reward -11.325607(0.000000)\n",
      "I0423 18:21:50.490918 17664 misc.py:27] episode 251, reward -5.064067, avg reward -2.481428, total steps 32128, episode step 128\n",
      "[2020-04-23 18:21:50,490] episode 251, reward -5.064067, avg reward -2.481428, total steps 32128, episode step 128\n",
      "I0423 18:21:57.833001 17664 misc.py:27] episode 252, reward -2.219732, avg reward -2.554113, total steps 32256, episode step 128\n",
      "[2020-04-23 18:21:57,833] episode 252, reward -2.219732, avg reward -2.554113, total steps 32256, episode step 128\n",
      "I0423 18:22:05.411288 17664 misc.py:27] episode 253, reward -0.951129, avg reward -2.530188, total steps 32384, episode step 128\n",
      "[2020-04-23 18:22:05,411] episode 253, reward -0.951129, avg reward -2.530188, total steps 32384, episode step 128\n",
      "I0423 18:22:13.053145 17664 misc.py:27] episode 254, reward -4.343326, avg reward -2.570994, total steps 32512, episode step 128\n",
      "[2020-04-23 18:22:13,053] episode 254, reward -4.343326, avg reward -2.570994, total steps 32512, episode step 128\n",
      "I0423 18:22:19.762485 17664 misc.py:27] episode 255, reward -4.274191, avg reward -2.503451, total steps 32640, episode step 128\n",
      "[2020-04-23 18:22:19,762] episode 255, reward -4.274191, avg reward -2.503451, total steps 32640, episode step 128\n",
      "I0423 18:22:26.967438 17664 misc.py:27] episode 256, reward -5.331657, avg reward -2.499184, total steps 32768, episode step 128\n",
      "[2020-04-23 18:22:26,967] episode 256, reward -5.331657, avg reward -2.499184, total steps 32768, episode step 128\n",
      "I0423 18:22:34.832621 17664 misc.py:27] episode 257, reward -2.085557, avg reward -2.483555, total steps 32896, episode step 128\n",
      "[2020-04-23 18:22:34,832] episode 257, reward -2.085557, avg reward -2.483555, total steps 32896, episode step 128\n",
      "I0423 18:22:42.009373 17664 misc.py:27] episode 258, reward -3.129926, avg reward -2.491738, total steps 33024, episode step 128\n",
      "[2020-04-23 18:22:42,009] episode 258, reward -3.129926, avg reward -2.491738, total steps 33024, episode step 128\n",
      "I0423 18:22:49.823917 17664 misc.py:27] episode 259, reward -8.998801, avg reward -2.560502, total steps 33152, episode step 128\n",
      "[2020-04-23 18:22:49,823] episode 259, reward -8.998801, avg reward -2.560502, total steps 33152, episode step 128\n",
      "I0423 18:22:57.831654 17664 misc.py:27] episode 260, reward -5.254609, avg reward -2.568885, total steps 33280, episode step 128\n",
      "[2020-04-23 18:22:57,831] episode 260, reward -5.254609, avg reward -2.568885, total steps 33280, episode step 128\n",
      "I0423 18:22:57.834548 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:22:57,834] Testing...\n",
      "I0423 18:22:58.651997 17664 misc.py:55] Avg reward -3.306685(0.000000)\n",
      "[2020-04-23 18:22:58,651] Avg reward -3.306685(0.000000)\n",
      "I0423 18:23:05.942092 17664 misc.py:27] episode 261, reward 4.199695, avg reward -2.551688, total steps 33408, episode step 128\n",
      "[2020-04-23 18:23:05,942] episode 261, reward 4.199695, avg reward -2.551688, total steps 33408, episode step 128\n",
      "I0423 18:23:13.980831 17664 misc.py:27] episode 262, reward 1.483162, avg reward -2.499695, total steps 33536, episode step 128\n",
      "[2020-04-23 18:23:13,980] episode 262, reward 1.483162, avg reward -2.499695, total steps 33536, episode step 128\n",
      "I0423 18:23:21.604973 17664 misc.py:27] episode 263, reward 0.998819, avg reward -2.476210, total steps 33664, episode step 128\n",
      "[2020-04-23 18:23:21,604] episode 263, reward 0.998819, avg reward -2.476210, total steps 33664, episode step 128\n",
      "I0423 18:23:29.306856 17664 misc.py:27] episode 264, reward 8.809785, avg reward -2.346598, total steps 33792, episode step 128\n",
      "[2020-04-23 18:23:29,306] episode 264, reward 8.809785, avg reward -2.346598, total steps 33792, episode step 128\n",
      "I0423 18:23:37.139309 17664 misc.py:27] episode 265, reward -4.459477, avg reward -2.317126, total steps 33920, episode step 128\n",
      "[2020-04-23 18:23:37,139] episode 265, reward -4.459477, avg reward -2.317126, total steps 33920, episode step 128\n",
      "I0423 18:23:45.598821 17664 misc.py:27] episode 266, reward -1.492448, avg reward -2.301651, total steps 34048, episode step 128\n",
      "[2020-04-23 18:23:45,598] episode 266, reward -1.492448, avg reward -2.301651, total steps 34048, episode step 128\n",
      "I0423 18:23:52.945426 17664 misc.py:27] episode 267, reward -3.311245, avg reward -2.306472, total steps 34176, episode step 128\n",
      "[2020-04-23 18:23:52,945] episode 267, reward -3.311245, avg reward -2.306472, total steps 34176, episode step 128\n",
      "I0423 18:24:01.640985 17664 misc.py:27] episode 268, reward -9.310783, avg reward -2.366927, total steps 34304, episode step 128\n",
      "[2020-04-23 18:24:01,640] episode 268, reward -9.310783, avg reward -2.366927, total steps 34304, episode step 128\n",
      "I0423 18:24:09.254647 17664 misc.py:27] episode 269, reward -1.723936, avg reward -2.343122, total steps 34432, episode step 128\n",
      "[2020-04-23 18:24:09,254] episode 269, reward -1.723936, avg reward -2.343122, total steps 34432, episode step 128\n",
      "I0423 18:24:16.440145 17664 misc.py:27] episode 270, reward -8.641789, avg reward -2.357565, total steps 34560, episode step 128\n",
      "[2020-04-23 18:24:16,440] episode 270, reward -8.641789, avg reward -2.357565, total steps 34560, episode step 128\n",
      "I0423 18:24:16.443726 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:24:16,443] Testing...\n",
      "I0423 18:24:17.125837 17664 misc.py:55] Avg reward -4.857260(0.000000)\n",
      "[2020-04-23 18:24:17,125] Avg reward -4.857260(0.000000)\n",
      "I0423 18:24:23.933914 17664 misc.py:27] episode 271, reward -3.487194, avg reward -2.368267, total steps 34688, episode step 128\n",
      "[2020-04-23 18:24:23,933] episode 271, reward -3.487194, avg reward -2.368267, total steps 34688, episode step 128\n",
      "I0423 18:24:30.830966 17664 misc.py:27] episode 272, reward -2.415848, avg reward -2.443186, total steps 34816, episode step 128\n",
      "[2020-04-23 18:24:30,830] episode 272, reward -2.415848, avg reward -2.443186, total steps 34816, episode step 128\n",
      "I0423 18:24:39.126954 17664 misc.py:27] episode 273, reward -10.268387, avg reward -2.535148, total steps 34944, episode step 128\n",
      "[2020-04-23 18:24:39,126] episode 273, reward -10.268387, avg reward -2.535148, total steps 34944, episode step 128\n",
      "I0423 18:24:47.020391 17664 misc.py:27] episode 274, reward -10.945114, avg reward -2.537448, total steps 35072, episode step 128\n",
      "[2020-04-23 18:24:47,020] episode 274, reward -10.945114, avg reward -2.537448, total steps 35072, episode step 128\n",
      "I0423 18:24:54.324713 17664 misc.py:27] episode 275, reward -5.945951, avg reward -2.575710, total steps 35200, episode step 128\n",
      "[2020-04-23 18:24:54,324] episode 275, reward -5.945951, avg reward -2.575710, total steps 35200, episode step 128\n",
      "I0423 18:25:01.817795 17664 misc.py:27] episode 276, reward 6.805680, avg reward -2.456541, total steps 35328, episode step 128\n",
      "[2020-04-23 18:25:01,817] episode 276, reward 6.805680, avg reward -2.456541, total steps 35328, episode step 128\n",
      "I0423 18:25:09.597753 17664 misc.py:27] episode 277, reward -7.874472, avg reward -2.449172, total steps 35456, episode step 128\n",
      "[2020-04-23 18:25:09,597] episode 277, reward -7.874472, avg reward -2.449172, total steps 35456, episode step 128\n",
      "I0423 18:25:19.351520 17664 misc.py:27] episode 278, reward -2.492903, avg reward -2.422832, total steps 35584, episode step 128\n",
      "[2020-04-23 18:25:19,351] episode 278, reward -2.492903, avg reward -2.422832, total steps 35584, episode step 128\n",
      "I0423 18:25:28.983217 17664 misc.py:27] episode 279, reward -2.487245, avg reward -2.427902, total steps 35712, episode step 128\n",
      "[2020-04-23 18:25:28,983] episode 279, reward -2.487245, avg reward -2.427902, total steps 35712, episode step 128\n",
      "I0423 18:25:37.261527 17664 misc.py:27] episode 280, reward -5.982312, avg reward -2.517755, total steps 35840, episode step 128\n",
      "[2020-04-23 18:25:37,261] episode 280, reward -5.982312, avg reward -2.517755, total steps 35840, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 18:25:37.270391 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:25:37,270] Testing...\n",
      "I0423 18:25:38.102364 17664 misc.py:55] Avg reward 0.962874(0.000000)\n",
      "[2020-04-23 18:25:38,102] Avg reward 0.962874(0.000000)\n",
      "I0423 18:25:46.966449 17664 misc.py:27] episode 281, reward -7.959420, avg reward -2.549606, total steps 35968, episode step 128\n",
      "[2020-04-23 18:25:46,966] episode 281, reward -7.959420, avg reward -2.549606, total steps 35968, episode step 128\n",
      "I0423 18:25:57.443083 17664 misc.py:27] episode 282, reward -5.892843, avg reward -2.699007, total steps 36096, episode step 128\n",
      "[2020-04-23 18:25:57,443] episode 282, reward -5.892843, avg reward -2.699007, total steps 36096, episode step 128\n",
      "I0423 18:26:06.978196 17664 misc.py:27] episode 283, reward -7.001473, avg reward -2.741372, total steps 36224, episode step 128\n",
      "[2020-04-23 18:26:06,978] episode 283, reward -7.001473, avg reward -2.741372, total steps 36224, episode step 128\n",
      "I0423 18:26:15.337965 17664 misc.py:27] episode 284, reward 4.596881, avg reward -2.620712, total steps 36352, episode step 128\n",
      "[2020-04-23 18:26:15,337] episode 284, reward 4.596881, avg reward -2.620712, total steps 36352, episode step 128\n",
      "I0423 18:26:24.368414 17664 misc.py:27] episode 285, reward -2.745058, avg reward -2.701900, total steps 36480, episode step 128\n",
      "[2020-04-23 18:26:24,368] episode 285, reward -2.745058, avg reward -2.701900, total steps 36480, episode step 128\n",
      "I0423 18:26:33.341764 17664 misc.py:27] episode 286, reward -0.912810, avg reward -2.687955, total steps 36608, episode step 128\n",
      "[2020-04-23 18:26:33,341] episode 286, reward -0.912810, avg reward -2.687955, total steps 36608, episode step 128\n",
      "I0423 18:26:43.273185 17664 misc.py:27] episode 287, reward -2.956452, avg reward -2.756683, total steps 36736, episode step 128\n",
      "[2020-04-23 18:26:43,273] episode 287, reward -2.956452, avg reward -2.756683, total steps 36736, episode step 128\n",
      "I0423 18:26:53.076287 17664 misc.py:27] episode 288, reward -3.948062, avg reward -2.767914, total steps 36864, episode step 128\n",
      "[2020-04-23 18:26:53,076] episode 288, reward -3.948062, avg reward -2.767914, total steps 36864, episode step 128\n",
      "I0423 18:27:01.368292 17664 misc.py:27] episode 289, reward 0.882632, avg reward -2.805306, total steps 36992, episode step 128\n",
      "[2020-04-23 18:27:01,368] episode 289, reward 0.882632, avg reward -2.805306, total steps 36992, episode step 128\n",
      "I0423 18:27:10.692610 17664 misc.py:27] episode 290, reward -12.025972, avg reward -2.963538, total steps 37120, episode step 128\n",
      "[2020-04-23 18:27:10,692] episode 290, reward -12.025972, avg reward -2.963538, total steps 37120, episode step 128\n",
      "I0423 18:27:10.695602 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:27:10,695] Testing...\n",
      "I0423 18:27:11.489229 17664 misc.py:55] Avg reward 5.242096(0.000000)\n",
      "[2020-04-23 18:27:11,489] Avg reward 5.242096(0.000000)\n",
      "I0423 18:27:20.857764 17664 misc.py:27] episode 291, reward -8.205236, avg reward -3.052499, total steps 37248, episode step 128\n",
      "[2020-04-23 18:27:20,857] episode 291, reward -8.205236, avg reward -3.052499, total steps 37248, episode step 128\n",
      "I0423 18:27:30.888825 17664 misc.py:27] episode 292, reward -0.222942, avg reward -3.040012, total steps 37376, episode step 128\n",
      "[2020-04-23 18:27:30,888] episode 292, reward -0.222942, avg reward -3.040012, total steps 37376, episode step 128\n",
      "I0423 18:27:40.844930 17664 misc.py:27] episode 293, reward -2.316936, avg reward -3.005143, total steps 37504, episode step 128\n",
      "[2020-04-23 18:27:40,844] episode 293, reward -2.316936, avg reward -3.005143, total steps 37504, episode step 128\n",
      "I0423 18:27:50.133928 17664 misc.py:27] episode 294, reward -5.834760, avg reward -3.003931, total steps 37632, episode step 128\n",
      "[2020-04-23 18:27:50,133] episode 294, reward -5.834760, avg reward -3.003931, total steps 37632, episode step 128\n",
      "I0423 18:27:59.571325 17664 misc.py:27] episode 295, reward -10.723425, avg reward -3.113292, total steps 37760, episode step 128\n",
      "[2020-04-23 18:27:59,571] episode 295, reward -10.723425, avg reward -3.113292, total steps 37760, episode step 128\n",
      "I0423 18:28:08.172656 17664 misc.py:27] episode 296, reward -1.362945, avg reward -3.062201, total steps 37888, episode step 128\n",
      "[2020-04-23 18:28:08,172] episode 296, reward -1.362945, avg reward -3.062201, total steps 37888, episode step 128\n",
      "I0423 18:28:17.996996 17664 misc.py:27] episode 297, reward -0.278990, avg reward -3.078990, total steps 38016, episode step 128\n",
      "[2020-04-23 18:28:17,996] episode 297, reward -0.278990, avg reward -3.078990, total steps 38016, episode step 128\n",
      "I0423 18:28:26.901746 17664 misc.py:27] episode 298, reward 3.277451, avg reward -3.014670, total steps 38144, episode step 128\n",
      "[2020-04-23 18:28:26,901] episode 298, reward 3.277451, avg reward -3.014670, total steps 38144, episode step 128\n",
      "I0423 18:28:35.122467 17664 misc.py:27] episode 299, reward -2.922432, avg reward -3.266307, total steps 38272, episode step 128\n",
      "[2020-04-23 18:28:35,122] episode 299, reward -2.922432, avg reward -3.266307, total steps 38272, episode step 128\n",
      "I0423 18:28:44.232431 17664 misc.py:27] episode 300, reward -4.190313, avg reward -3.235041, total steps 38400, episode step 128\n",
      "[2020-04-23 18:28:44,232] episode 300, reward -4.190313, avg reward -3.235041, total steps 38400, episode step 128\n",
      "I0423 18:28:44.239665 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:28:44,239] Testing...\n",
      "I0423 18:28:45.058942 17664 misc.py:55] Avg reward -2.763204(0.000000)\n",
      "[2020-04-23 18:28:45,058] Avg reward -2.763204(0.000000)\n",
      "I0423 18:28:53.888350 17664 misc.py:27] episode 301, reward 1.445982, avg reward -3.221963, total steps 38528, episode step 128\n",
      "[2020-04-23 18:28:53,888] episode 301, reward 1.445982, avg reward -3.221963, total steps 38528, episode step 128\n",
      "I0423 18:29:02.714233 17664 misc.py:27] episode 302, reward -4.105877, avg reward -3.206740, total steps 38656, episode step 128\n",
      "[2020-04-23 18:29:02,714] episode 302, reward -4.105877, avg reward -3.206740, total steps 38656, episode step 128\n",
      "I0423 18:29:11.112167 17664 misc.py:27] episode 303, reward -5.857558, avg reward -3.239890, total steps 38784, episode step 128\n",
      "[2020-04-23 18:29:11,112] episode 303, reward -5.857558, avg reward -3.239890, total steps 38784, episode step 128\n",
      "I0423 18:29:19.062303 17664 misc.py:27] episode 304, reward -4.253514, avg reward -3.307683, total steps 38912, episode step 128\n",
      "[2020-04-23 18:29:19,062] episode 304, reward -4.253514, avg reward -3.307683, total steps 38912, episode step 128\n",
      "I0423 18:29:27.881244 17664 misc.py:27] episode 305, reward 2.452397, avg reward -3.255764, total steps 39040, episode step 128\n",
      "[2020-04-23 18:29:27,881] episode 305, reward 2.452397, avg reward -3.255764, total steps 39040, episode step 128\n",
      "I0423 18:29:36.948055 17664 misc.py:27] episode 306, reward -2.845980, avg reward -3.268571, total steps 39168, episode step 128\n",
      "[2020-04-23 18:29:36,948] episode 306, reward -2.845980, avg reward -3.268571, total steps 39168, episode step 128\n",
      "I0423 18:29:45.511507 17664 misc.py:27] episode 307, reward -4.407530, avg reward -3.259138, total steps 39296, episode step 128\n",
      "[2020-04-23 18:29:45,511] episode 307, reward -4.407530, avg reward -3.259138, total steps 39296, episode step 128\n",
      "I0423 18:29:50.847824 17664 misc.py:27] episode 308, reward -1.379942, avg reward -3.194437, total steps 39424, episode step 128\n",
      "[2020-04-23 18:29:50,847] episode 308, reward -1.379942, avg reward -3.194437, total steps 39424, episode step 128\n",
      "I0423 18:29:57.632543 17664 misc.py:27] episode 309, reward -3.714916, avg reward -3.208798, total steps 39552, episode step 128\n",
      "[2020-04-23 18:29:57,632] episode 309, reward -3.714916, avg reward -3.208798, total steps 39552, episode step 128\n",
      "I0423 18:30:03.199187 17664 misc.py:27] episode 310, reward -6.024848, avg reward -3.256713, total steps 39680, episode step 128\n",
      "[2020-04-23 18:30:03,199] episode 310, reward -6.024848, avg reward -3.256713, total steps 39680, episode step 128\n",
      "I0423 18:30:03.202179 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:30:03,202] Testing...\n",
      "I0423 18:30:03.911540 17664 misc.py:55] Avg reward -5.921720(0.000000)\n",
      "[2020-04-23 18:30:03,911] Avg reward -5.921720(0.000000)\n",
      "I0423 18:30:09.518385 17664 misc.py:27] episode 311, reward -3.017566, avg reward -3.236344, total steps 39808, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-23 18:30:09,518] episode 311, reward -3.017566, avg reward -3.236344, total steps 39808, episode step 128\n",
      "I0423 18:30:15.151499 17664 misc.py:27] episode 312, reward -2.770409, avg reward -3.206015, total steps 39936, episode step 128\n",
      "[2020-04-23 18:30:15,151] episode 312, reward -2.770409, avg reward -3.206015, total steps 39936, episode step 128\n",
      "I0423 18:30:19.900205 17664 misc.py:27] episode 313, reward 0.750926, avg reward -3.195901, total steps 40064, episode step 128\n",
      "[2020-04-23 18:30:19,900] episode 313, reward 0.750926, avg reward -3.195901, total steps 40064, episode step 128\n",
      "I0423 18:30:24.675281 17664 misc.py:27] episode 314, reward -7.463086, avg reward -3.162997, total steps 40192, episode step 128\n",
      "[2020-04-23 18:30:24,675] episode 314, reward -7.463086, avg reward -3.162997, total steps 40192, episode step 128\n",
      "I0423 18:30:29.289466 17664 misc.py:27] episode 315, reward -4.312718, avg reward -3.245922, total steps 40320, episode step 128\n",
      "[2020-04-23 18:30:29,289] episode 315, reward -4.312718, avg reward -3.245922, total steps 40320, episode step 128\n",
      "I0423 18:30:33.900451 17664 misc.py:27] episode 316, reward -3.387486, avg reward -3.245292, total steps 40448, episode step 128\n",
      "[2020-04-23 18:30:33,900] episode 316, reward -3.387486, avg reward -3.245292, total steps 40448, episode step 128\n",
      "I0423 18:30:39.061707 17664 misc.py:27] episode 317, reward -5.778798, avg reward -3.308594, total steps 40576, episode step 128\n",
      "[2020-04-23 18:30:39,061] episode 317, reward -5.778798, avg reward -3.308594, total steps 40576, episode step 128\n",
      "I0423 18:30:44.034587 17664 misc.py:27] episode 318, reward -6.959049, avg reward -3.307155, total steps 40704, episode step 128\n",
      "[2020-04-23 18:30:44,034] episode 318, reward -6.959049, avg reward -3.307155, total steps 40704, episode step 128\n",
      "I0423 18:30:48.908266 17664 misc.py:27] episode 319, reward 1.628959, avg reward -3.266539, total steps 40832, episode step 128\n",
      "[2020-04-23 18:30:48,908] episode 319, reward 1.628959, avg reward -3.266539, total steps 40832, episode step 128\n",
      "I0423 18:30:54.152884 17664 misc.py:27] episode 320, reward -2.749001, avg reward -3.311163, total steps 40960, episode step 128\n",
      "[2020-04-23 18:30:54,152] episode 320, reward -2.749001, avg reward -3.311163, total steps 40960, episode step 128\n",
      "I0423 18:30:54.158971 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:30:54,158] Testing...\n",
      "I0423 18:30:54.890087 17664 misc.py:55] Avg reward 2.511295(0.000000)\n",
      "[2020-04-23 18:30:54,890] Avg reward 2.511295(0.000000)\n",
      "I0423 18:31:00.216165 17664 misc.py:27] episode 321, reward -3.948999, avg reward -3.319185, total steps 41088, episode step 128\n",
      "[2020-04-23 18:31:00,216] episode 321, reward -3.948999, avg reward -3.319185, total steps 41088, episode step 128\n",
      "I0423 18:31:05.006142 17664 misc.py:27] episode 322, reward -5.279037, avg reward -3.301804, total steps 41216, episode step 128\n",
      "[2020-04-23 18:31:05,006] episode 322, reward -5.279037, avg reward -3.301804, total steps 41216, episode step 128\n",
      "I0423 18:31:10.000668 17664 misc.py:27] episode 323, reward -3.136768, avg reward -3.285718, total steps 41344, episode step 128\n",
      "[2020-04-23 18:31:10,000] episode 323, reward -3.136768, avg reward -3.285718, total steps 41344, episode step 128\n",
      "I0423 18:31:14.833639 17664 misc.py:27] episode 324, reward -5.559281, avg reward -3.442482, total steps 41472, episode step 128\n",
      "[2020-04-23 18:31:14,833] episode 324, reward -5.559281, avg reward -3.442482, total steps 41472, episode step 128\n",
      "I0423 18:31:19.696572 17664 misc.py:27] episode 325, reward -6.939670, avg reward -3.459695, total steps 41600, episode step 128\n",
      "[2020-04-23 18:31:19,696] episode 325, reward -6.939670, avg reward -3.459695, total steps 41600, episode step 128\n",
      "I0423 18:31:24.353920 17664 misc.py:27] episode 326, reward -7.824970, avg reward -3.521880, total steps 41728, episode step 128\n",
      "[2020-04-23 18:31:24,353] episode 326, reward -7.824970, avg reward -3.521880, total steps 41728, episode step 128\n",
      "I0423 18:31:29.298641 17664 misc.py:27] episode 327, reward -4.093827, avg reward -3.509013, total steps 41856, episode step 128\n",
      "[2020-04-23 18:31:29,298] episode 327, reward -4.093827, avg reward -3.509013, total steps 41856, episode step 128\n",
      "I0423 18:31:33.871441 17664 misc.py:27] episode 328, reward -3.513920, avg reward -3.485775, total steps 41984, episode step 128\n",
      "[2020-04-23 18:31:33,871] episode 328, reward -3.513920, avg reward -3.485775, total steps 41984, episode step 128\n",
      "I0423 18:31:38.872485 17664 misc.py:27] episode 329, reward -1.973569, avg reward -3.445481, total steps 42112, episode step 128\n",
      "[2020-04-23 18:31:38,872] episode 329, reward -1.973569, avg reward -3.445481, total steps 42112, episode step 128\n",
      "I0423 18:31:43.798031 17664 misc.py:27] episode 330, reward -3.295861, avg reward -3.394316, total steps 42240, episode step 128\n",
      "[2020-04-23 18:31:43,798] episode 330, reward -3.295861, avg reward -3.394316, total steps 42240, episode step 128\n",
      "I0423 18:31:43.801025 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:31:43,801] Testing...\n",
      "I0423 18:31:44.217991 17664 misc.py:55] Avg reward 2.923813(0.000000)\n",
      "[2020-04-23 18:31:44,217] Avg reward 2.923813(0.000000)\n",
      "I0423 18:31:49.137612 17664 misc.py:27] episode 331, reward -12.627544, avg reward -3.538439, total steps 42368, episode step 128\n",
      "[2020-04-23 18:31:49,137] episode 331, reward -12.627544, avg reward -3.538439, total steps 42368, episode step 128\n",
      "I0423 18:31:53.770323 17664 misc.py:27] episode 332, reward -3.716495, avg reward -3.539224, total steps 42496, episode step 128\n",
      "[2020-04-23 18:31:53,770] episode 332, reward -3.716495, avg reward -3.539224, total steps 42496, episode step 128\n",
      "I0423 18:31:58.641401 17664 misc.py:27] episode 333, reward 2.172052, avg reward -3.489138, total steps 42624, episode step 128\n",
      "[2020-04-23 18:31:58,641] episode 333, reward 2.172052, avg reward -3.489138, total steps 42624, episode step 128\n",
      "I0423 18:32:03.486236 17664 misc.py:27] episode 334, reward -5.576671, avg reward -3.570494, total steps 42752, episode step 128\n",
      "[2020-04-23 18:32:03,486] episode 334, reward -5.576671, avg reward -3.570494, total steps 42752, episode step 128\n",
      "I0423 18:32:08.259149 17664 misc.py:27] episode 335, reward -5.974663, avg reward -3.579216, total steps 42880, episode step 128\n",
      "[2020-04-23 18:32:08,259] episode 335, reward -5.974663, avg reward -3.579216, total steps 42880, episode step 128\n",
      "I0423 18:32:13.107996 17664 misc.py:27] episode 336, reward -9.128971, avg reward -3.582731, total steps 43008, episode step 128\n",
      "[2020-04-23 18:32:13,107] episode 336, reward -9.128971, avg reward -3.582731, total steps 43008, episode step 128\n",
      "I0423 18:32:17.912348 17664 misc.py:27] episode 337, reward -2.496193, avg reward -3.597198, total steps 43136, episode step 128\n",
      "[2020-04-23 18:32:17,912] episode 337, reward -2.496193, avg reward -3.597198, total steps 43136, episode step 128\n",
      "I0423 18:32:22.562556 17664 misc.py:27] episode 338, reward -6.573009, avg reward -3.639229, total steps 43264, episode step 128\n",
      "[2020-04-23 18:32:22,562] episode 338, reward -6.573009, avg reward -3.639229, total steps 43264, episode step 128\n",
      "I0423 18:32:27.240483 17664 misc.py:27] episode 339, reward -2.293558, avg reward -3.652136, total steps 43392, episode step 128\n",
      "[2020-04-23 18:32:27,240] episode 339, reward -2.293558, avg reward -3.652136, total steps 43392, episode step 128\n",
      "I0423 18:32:32.073049 17664 misc.py:27] episode 340, reward -2.452864, avg reward -3.607643, total steps 43520, episode step 128\n",
      "[2020-04-23 18:32:32,073] episode 340, reward -2.452864, avg reward -3.607643, total steps 43520, episode step 128\n",
      "I0423 18:32:32.075044 17664 misc.py:47] Testing...\n",
      "[2020-04-23 18:32:32,075] Testing...\n",
      "I0423 18:32:32.493588 17664 misc.py:55] Avg reward -13.972808(0.000000)\n",
      "[2020-04-23 18:32:32,493] Avg reward -13.972808(0.000000)\n",
      "I0423 18:32:37.941584 17664 misc.py:27] episode 341, reward -3.873233, avg reward -3.590306, total steps 43648, episode step 128\n",
      "[2020-04-23 18:32:37,941] episode 341, reward -3.873233, avg reward -3.590306, total steps 43648, episode step 128\n",
      "I0423 18:32:43.238882 17664 misc.py:27] episode 342, reward -8.643971, avg reward -3.657812, total steps 43776, episode step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-23 18:32:43,238] episode 342, reward -8.643971, avg reward -3.657812, total steps 43776, episode step 128\n",
      "I0423 18:32:48.493866 17664 misc.py:27] episode 343, reward -4.591731, avg reward -3.696953, total steps 43904, episode step 128\n",
      "[2020-04-23 18:32:48,493] episode 343, reward -4.591731, avg reward -3.696953, total steps 43904, episode step 128\n"
     ]
    }
   ],
   "source": [
    "from DeepRL.utils import run_episodes\n",
    "agent.task._plot = agent.task._plot2 = None\n",
    "try:    \n",
    "    run_episodes(agent)\n",
    "except KeyboardInterrupt as e:\n",
    "    save_ddpg(agent)\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:16.462355Z",
     "start_time": "2018-02-18T07:56:16.094057Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "plt.figure()\n",
    "df_online, df = load_stats_ddpg(agent)\n",
    "sns.regplot(x=\"step\", y=\"rewards\", data=df_online, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:23.350929Z",
     "start_time": "2018-02-18T07:56:23.316549Z"
    }
   },
   "outputs": [],
   "source": [
    "# monthly growth\n",
    "portfolio_return = (1+df_online.rewards[-100:].mean())\n",
    "\n",
    "returns = task.unwrapped.src.data[0,:,:1]\n",
    "market_return = (1+returns).mean()\n",
    "market_return, portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:29:04.356761Z",
     "start_time": "2018-02-18T03:29:04.327173Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-30T00:20:53.430Z"
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:37:21.522568Z",
     "start_time": "2018-02-18T03:37:21.454751Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:24.491787Z",
     "start_time": "2018-02-18T07:56:24.390618Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_algo(env, algo, seed=0):\n",
    "    \"\"\"\n",
    "    Runs and algo from https://github.com/Marigold/universal-portfolios on env\n",
    "    \n",
    "    https://github.com/Marigold/universal-portfolios/commit/e8970a82427522ef11b1c3cbf681e18b5fe8169c\n",
    "    \"\"\"\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    state = env.reset()\n",
    "    for i in range(env.unwrapped.sim.steps):\n",
    "        \n",
    "        history= pd.DataFrame(state[0,:,:], columns=env.unwrapped.src.asset_names)\n",
    "        # MPT wants a cash column, and it should be first\n",
    "        history['CASH']=1\n",
    "        history=history[['CASH'] + env.unwrapped.src.asset_names]\n",
    "#         cols = list(history.columns)\n",
    "#         cols[0]='CASH'\n",
    "#         history.columns = cols\n",
    "        \n",
    "        x=history.iloc[-1]\n",
    "        \n",
    "        last_b = env.unwrapped.sim.w0#[1:]\n",
    "\n",
    "        algo.init_step(history)\n",
    "        # some don't want history\n",
    "        try:\n",
    "            action = algo.step(x, last_b, history)\n",
    "        except TypeError:\n",
    "            action = algo.step(x, last_b)\n",
    "        \n",
    "        # might by dataframe\n",
    "        action = getattr(action, 'value', action)\n",
    "        \n",
    "        # For upt\n",
    "        if isinstance(action, np.matrixlib.defmatrix.matrix):\n",
    "            action = np.array(action.tolist()).T[0]\n",
    "            \n",
    "        \n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break   \n",
    "    df = pd.DataFrame(env.unwrapped.infos)\n",
    "    df.index = pd.to_datetime(df['date']*1e9)\n",
    "    return df['portfolio_value'], df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:25.669367Z",
     "start_time": "2018-02-18T07:56:24.530860Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use test env\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "test_steps=5000\n",
    "env_test = task_fn_test()\n",
    "agent.task = env_test\n",
    "agent.config.max_episode_length = test_steps\n",
    "agent.task.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "# run in deterministic mode, no training, no exploration\n",
    "agent.episode(True)\n",
    "agent.task.render('notebook')\n",
    "agent.task.render('notebook', True)\n",
    "\n",
    "df = pd.DataFrame(agent.task.unwrapped.infos)\n",
    "df.index = pd.to_datetime(df['date']*1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T04:01:49.504199Z",
     "start_time": "2018-02-18T04:01:49.467708Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:34:12.858016Z",
     "start_time": "2018-02-18T03:34:12.772885Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T07:56:29.025682Z",
     "start_time": "2018-02-18T07:56:25.704036Z"
    }
   },
   "outputs": [],
   "source": [
    "from UniversalPortfolios.universal import algos\n",
    "env = task.unwrapped\n",
    "price_cols = [col for col in df.columns if col.startswith('price')]\n",
    "for col in price_cols:\n",
    "    df[col]=df[col].cumprod()\n",
    "\n",
    "df = df[price_cols + ['portfolio_value']]\n",
    "    \n",
    "algo_dict=dict(\n",
    "    # Pick the same is in https://arxiv.org/pdf/1706.10059.pdf\n",
    "    # Benchmarks\n",
    "#     UCRP=algos.UP(),\n",
    "    \n",
    "    # Follow the winner\n",
    "    BestSoFar=algos.BestSoFar(cov_window=env_test.unwrapped.src.window_length-1),\n",
    "#     UniversalPortfolio=algos.UP(eval_points=1000),\n",
    "    ONS=algos.ONS(),\n",
    "    \n",
    "    # Follow the loser\n",
    "#     OnlineMovingAverageReversion=algos.OLMAR(window=env.src.window_length-1, eps=10), \n",
    "    RMR=algos.RMR(window=env_test.unwrapped.src.window_length-1, eps=10),\n",
    "#     PassiveAggressiveMeanReversion=algos.PAMR(),\n",
    "    \n",
    "    # Pattern matching\n",
    "    #     CorrelationDrivenNonparametricLearning=algos.CORN(window=30),\n",
    ")\n",
    "for name, algo in algo_dict.items():\n",
    "    print(name)\n",
    "    perf, _ = test_algo(env_test, algo)\n",
    "    perf.index=df.index\n",
    "    df[name]=perf\n",
    "\n",
    "# put portfolio value at end so we plot it on top and can therefore see it\n",
    "cols = list(df.columns.drop('portfolio_value'))+['portfolio_value']\n",
    "df=df[cols]\n",
    "\n",
    "\n",
    "df.plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:53:15.784969Z",
     "start_time": "2018-02-18T03:53:15.660001Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "70px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "from ES import sepCEM, Control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# numeric\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "from util import to_numpy\n",
    "\n",
    "\n",
    "# logging\n",
    "logger = log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)\n",
    "logging.basicConfig()\n",
    "log.info('%s logger started.', __name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.abspath('.'))\n",
    "os.sys.path.append(os.path.abspath('DeepRL'))\n",
    "get_ipython().run_line_magic('reload_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 50\n",
    "steps = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "ts = datetime.datetime.utcnow().strftime('%Y%m%d_%H-%M-%S')\n",
    "\n",
    "save_path = './outputs/pytorch-DDPG/pytorch-DDPG-EIIE-action-crypto-%s.model' % ts\n",
    "print(save_path)\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard_logger import configure, log_value\n",
    "tag = 'ddpg-' + ts\n",
    "print('tensorboard --logdir '+\"runs/\" + tag)\n",
    "try:\n",
    "    configure(\"runs/\" + tag)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_portfolio_management.environments.portfolio import PortfolioEnv\n",
    "from rl_portfolio_management.util import MDD, sharpe, softmax\n",
    "from rl_portfolio_management.wrappers import SoftmaxActions, TransposeHistory, ConcatStates\n",
    "\n",
    "df_train = pd.read_hdf('./data/poloniex_30m.hf',key='train')\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "class DeepRLWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.render_on_reset = False\n",
    "        \n",
    "        self.state_dim = self.observation_space.shape\n",
    "        self.action_dim = self.action_space.shape[0]\n",
    "        \n",
    "        self.name = 'PortfolioEnv'\n",
    "        self.success_threshold = 2\n",
    "        \n",
    "    def normalize_state(self, state):\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info =self.env.step(action)\n",
    "        reward*=1e4 # often reward scaling is important sooo...\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def reset(self):        \n",
    "        # here's a roundabout way to get it to plot on reset\n",
    "        if self.render_on_reset: \n",
    "            self.env.render('notebook')\n",
    "\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_fn():\n",
    "    env = PortfolioEnv(df=df_train, steps=steps, output_mode='EIIE')\n",
    "    env = TransposeHistory(env)\n",
    "    env = ConcatStates(env)\n",
    "    env = SoftmaxActions(env)\n",
    "    env = DeepRLWrapper(env)\n",
    "    return env\n",
    "\n",
    "def task_fn_test():\n",
    "    env = PortfolioEnv(df=df_test, steps=steps, output_mode='EIIE')\n",
    "    env = TransposeHistory(env)\n",
    "    env = ConcatStates(env)\n",
    "    env = SoftmaxActions(env)\n",
    "    env = DeepRLWrapper(env)\n",
    "    return env\n",
    "    \n",
    "# sanity check\n",
    "task = task_fn()\n",
    "task.reset().shape, task.step(task.action_space.sample())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "\n",
    "def save_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    save_file = 'data/%s-%s-model-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    agent.save(save_file)\n",
    "    print(save_file)\n",
    "    \n",
    "\n",
    "def load_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    save_file = 'data/%s-%s-model-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "    new_states = pickle.load(open(save_file, 'rb'))\n",
    "    states = agent.worker_network.load_state_dict(new_states)\n",
    "\n",
    "\n",
    "def load_stats_ddpg(agent):\n",
    "    agent_type = agent.__class__.__name__\n",
    "    online_stats_file = 'data/%s-%s-online-stats-%s.bin' % (\n",
    "                    agent_type, config.tag, agent.task.name)\n",
    "    try:\n",
    "        steps, rewards = pickle.load(open(online_stats_file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        steps =[]\n",
    "        rewards=[]\n",
    "    df_online = pd.DataFrame(np.array([steps, rewards]).T, columns=['steps','rewards'])\n",
    "    if len(df_online):\n",
    "        df_online['step'] = df_online['steps'].cumsum()\n",
    "        df_online.index.name = 'episodes'\n",
    "    \n",
    "    stats_file = 'data/%s-%s-all-stats-%s.bin' % (agent_type, config.tag, agent.task.name)\n",
    "\n",
    "    try:\n",
    "        stats = pickle.load(open(stats_file, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        stats = {}\n",
    "    df = pd.DataFrame(stats[\"test_rewards\"], columns=['rewards'])\n",
    "    if len(df):\n",
    "#         df[\"steps\"]=range(len(df))*50\n",
    "\n",
    "        df.index.name = 'episodes'\n",
    "    return df_online, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from DeepRL.agent import ProximalPolicyOptimization\n",
    "from DeepRL.network import DisjointActorCriticNet #, DeterministicActorNet, DeterministicCriticNet\n",
    "from DeepRL.component import GaussianPolicy, HighDimActionReplay, OrnsteinUhlenbeckProcess\n",
    "from DeepRL.utils import Config, Logger\n",
    "import gym\n",
    "import torch\n",
    "gym.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepRL.utils.normalizer import Normalizer\n",
    "\n",
    "null_normaliser = lambda x:x\n",
    "\n",
    "# USE_CUDA = torch.cuda.is_available()\n",
    "# if USE_CUDA:\n",
    "#     print(\"using CUDA\")\n",
    "#     FloatTensor = torch.cuda.FloatTensor\n",
    "# else:\n",
    "#     print(\"using CPU\")\n",
    "#     FloatTensor = torch.FloatTensor\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(actor, env, max_steps, memory=None, n_episodes=1, random=False, noise=None):\n",
    "    \"\"\"\n",
    "    Computes the score of an actor on a given number of runs,\n",
    "    fills the memory if needed\n",
    "    \"\"\"\n",
    "\n",
    "    if not random:\n",
    "        def policy(state):\n",
    "            \n",
    "            state = torch.FloatTensor(np.array([state])) #.reshape(-1))\n",
    "            # print(\"Action Shape: \", np.shape(state))\n",
    "            action = actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "            if noise is not None:\n",
    "                action += noise.sample()\n",
    "\n",
    "            max_action = int(env.action_space.high[0])\n",
    "            return np.clip(action, -max_action, max_action)\n",
    "\n",
    "    else:\n",
    "        def policy(state):\n",
    "            return env.action_space.sample()\n",
    "\n",
    "    scores = []\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "\n",
    "        score = 0\n",
    "        obs = deepcopy(env.reset())\n",
    "        #  experiences = self.replay.sample()\n",
    "        # states, actions, rewards, next_states, terminals = experiences\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # get next action and act\n",
    "            action = policy(obs)\n",
    "            n_obs, reward, done, _ = env.step(action)\n",
    "            done_bool = 0 if steps + \\\n",
    "                1 == max_steps else float(done)\n",
    "            score += reward\n",
    "            steps += 1\n",
    "\n",
    "            # adding in memory\n",
    "            if memory is not None:\n",
    "                memory.feed((obs, action, reward, n_obs, done_bool))\n",
    "            obs = n_obs\n",
    "\n",
    "            # # render if needed\n",
    "            # if render:\n",
    "            #     env.render()\n",
    "\n",
    "            # reset when done\n",
    "            if done:\n",
    "                env.reset()\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores), steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.worker_network = config.network_fn()\n",
    "        self.target_network = config.network_fn()\n",
    "        self.target_network.load_state_dict(self.worker_network.state_dict())\n",
    "        self.actor_opt = config.actor_optimizer_fn(self.worker_network.actor.parameters())\n",
    "        self.critic_opt = config.critic_optimizer_fn(self.worker_network.critic.parameters())\n",
    "        self.replay = config.replay_fn()\n",
    "        self.random_process = config.random_process_fn()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.total_steps = 0\n",
    "        self.sigma_init = 1e-3\n",
    "        self.damp = 1e-3\n",
    "        self.damp_limit = 1e-5\n",
    "        self.pop_size = 10\n",
    "        self.elitism = 'elitism'\n",
    "        self.n_grad = 5\n",
    "        self.start_steps = 1000 #10000\n",
    "        self.n_episodes = 1\n",
    "        self.n_noisy = 0\n",
    "\n",
    "        self.state_normalizer = Normalizer(self.task.state_dim) # null_normaliser # \n",
    "        self.reward_normalizer = Normalizer(1)\n",
    "\n",
    "        self.es = sepCEM(self.worker_network.actor.get_size(), mu_init=self.worker_network.actor.get_params(), \n",
    "            sigma_init=self.sigma_init, damp=self.damp, damp_limit=self.damp_limit,\n",
    "            pop_size=self.pop_size, antithetic=not self.pop_size % 2, parents=self.pop_size // 2,\n",
    "            elitism=self.elitism)\n",
    "\n",
    "    def soft_update(self, target, src):\n",
    "        for target_param, param in zip(target.parameters(), src.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.config.target_network_mix) +\n",
    "                                    param.data * self.config.target_network_mix)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            torch.save(self.worker_network.state_dict(), f)\n",
    "\n",
    "    def episode(self, deterministic=False, video_recorder=None):\n",
    "        self.random_process.reset_states()\n",
    "        state = self.task.reset()\n",
    "        state = self.state_normalizer(state)\n",
    "\n",
    "        config = self.config\n",
    "        actor = self.worker_network.actor\n",
    "        actor_t = self.worker_network.actor\n",
    "        critic = self.worker_network.critic\n",
    "        critic_t = self.worker_network.critic\n",
    "        # target_actor = self.target_network.actor\n",
    "        # target_critic = self.target_network.critic\n",
    "\n",
    "        # Initialize our fitness and evolutionary params\n",
    "       \n",
    "        # fitness_ = []\n",
    "\n",
    "        step_cpt = 0\n",
    "        actor_steps = 0\n",
    "        #total_reward = 0.0\n",
    "        while True:\n",
    "\n",
    "            fitness = []\n",
    "            es_params = self.es.ask(self.pop_size)\n",
    "\n",
    "            # udpate the rl actors and the critic\n",
    "            if self.total_steps > self.start_steps:\n",
    "\n",
    "                for i in range(self.n_grad):\n",
    "\n",
    "                    # set params\n",
    "                    actor.set_params(es_params[i])\n",
    "                    actor_t.set_params(es_params[i])\n",
    "                    actor.optimizer = self.actor_opt\n",
    "\n",
    "                    # critic update\n",
    "                    for _ in range(actor_steps // self.n_grad):\n",
    "                        critic.update(self.replay, actor, critic_t)\n",
    "\n",
    "                    # actor update\n",
    "                    for _ in range(actor_steps):\n",
    "                        actor.update(self.replay, critic, actor_t)\n",
    "\n",
    "                    # get the params back in the population\n",
    "                    es_params[i] = actor.get_params()   \n",
    "            actor_steps = 0\n",
    "\n",
    "            # evaluate noisy actor(s)\n",
    "            for i in range(self.n_noisy):\n",
    "                actor.set_params(es_params[i])\n",
    "                f, steps = evaluate(actor, self.task, self.config.max_episode_length, \n",
    "                                    memory=self.replay, n_episodes=self.n_episodes, noise=self.random_process)\n",
    "                actor_steps += steps\n",
    "                #print('Noisy actor {} fitness:{}'.format(i, f))\n",
    "\n",
    "            # evaluate all actors\n",
    "            for params in es_params:\n",
    "\n",
    "                actor.set_params(params)\n",
    "                f, steps = evaluate(actor, self.task, self.config.max_episode_length, \n",
    "                                    memory=self.replay, n_episodes=self.n_episodes)\n",
    "                actor_steps += steps\n",
    "                fitness.append(f)\n",
    "\n",
    "                # print scores\n",
    "                #print('Actor fitness: {}'.format(f))\n",
    "\n",
    "            # update es\n",
    "            self.es.tell(es_params, fitness)\n",
    "\n",
    "            # update step counts\n",
    "            # self.total_steps += actor_steps\n",
    "            step_cpt += actor_steps\n",
    "\n",
    "\n",
    "            # actor.eval()\n",
    "            # action = actor.predict(np.stack([state])).flatten()\n",
    "            # if not deterministic:\n",
    "            #     action += self.random_process.sample()\n",
    "            # next_state, reward, done, info = self.task.step(action)\n",
    "            # if video_recorder is not None:\n",
    "            #     video_recorder.capture_frame()\n",
    "            # print(\"Done before done:\", done)\n",
    "            done = config.max_episode_length and self.total_steps >= config.max_episode_length\n",
    "            # next_state = self.state_normalizer(next_state) * config.reward_scaling\n",
    "            # total_reward += reward\n",
    "            \n",
    "#             # tensorboard logging\n",
    "#             prefix = 'test_' if deterministic else ''\n",
    "#             log_value(prefix + 'reward', reward, self.total_steps)\n",
    "# #             log_value(prefix + 'action', action, steps)\n",
    "#             log_value('memory_size', self.replay.size(), self.total_steps)     \n",
    "#             for key in info:\n",
    "#                 log_value(key, info[key], self.total_steps)     \n",
    "            \n",
    "            # reward = self.reward_normalizer(reward)\n",
    "\n",
    "            # if not deterministic:\n",
    "            #     self.replay.feed([state, action, reward, next_state, int(done)])\n",
    "            #     self.total_steps += 1\n",
    "\n",
    "            if self.total_steps % 10 == 0:\n",
    "                print(\"Total Steps:\", self.total_steps, \" Average fitness:\", np.mean(fitness))\n",
    "            self.total_steps += 1\n",
    "            # state = next_state\n",
    "\n",
    "            if done:\n",
    "                # print(\"max_ep_length:\", config.max_epsiode_length)\n",
    "                # print(\"total_steps:\", self.total_steps)\n",
    "                break\n",
    "            \n",
    "        return np.mean(fitness), step_cpt \n",
    "\n",
    "\n",
    "            # TODO Check what we might need from this\n",
    "            # if not deterministic and self.replay.size() >= config.min_memory_size:\n",
    "            #     self.worker_network.train()\n",
    "            #     experiences = self.replay.sample()\n",
    "            #     states, actions, rewards, next_states, terminals = experiences\n",
    "            #     q_next = target_critic.predict(next_states, target_actor.predict(next_states))\n",
    "            #     terminals = critic.to_torch_variable(terminals).unsqueeze(1)\n",
    "            #     rewards = critic.to_torch_variable(rewards).unsqueeze(1)\n",
    "            #     q_next = config.discount * q_next * (1 - terminals)\n",
    "            #     q_next.add_(rewards)\n",
    "            #     q_next = q_next.detach()\n",
    "            #     q = critic.predict(states, actions)\n",
    "            #     critic_loss = self.criterion(q, q_next)\n",
    "\n",
    "                #\n",
    "                # critic.zero_grad()\n",
    "                # self.critic_opt.zero_grad()\n",
    "                # critic_loss.backward()\n",
    "                # if config.gradient_clip:\n",
    "                #     grad_critic = nn.utils.clip_grad_norm(self.worker_network.parameters(), config.gradient_clip)\n",
    "                # self.critic_opt.step()\n",
    "\n",
    "                # actions = actor.predict(states, False)\n",
    "                # var_actions = Variable(actions.data, requires_grad=True)\n",
    "                # q = critic.predict(states, var_actions)\n",
    "                # q.backward(torch.ones(q.size()))\n",
    "\n",
    "                # actor.zero_grad()\n",
    "                # self.actor_opt.zero_grad()\n",
    "                # actions.backward(-var_actions.grad.data)\n",
    "                # if config.gradient_clip:\n",
    "                #     grad_actor = nn.utils.clip_grad_norm(self.worker_network.parameters(), config.gradient_clip)\n",
    "                # self.actor_opt.step()\n",
    "                \n",
    "                # tensorboard logging\n",
    "                # log_value('critic_loss', critic_loss.cpu().data.numpy().squeeze(), self.total_steps)\n",
    "                # log_value('loss_action', -q.sum(), self.total_steps)\n",
    "                # if config.gradient_clip:\n",
    "                #     log_value('grad_critic', grad_critic, self.total_steps)\n",
    "                #     log_value('grad_actor', grad_actor, self.total_steps)\n",
    "\n",
    "                # self.soft_update(self.target_network, self.worker_network)\n",
    "\n",
    "         #total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.state_dim, task.action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepRL.network.base_network import BasicNet\n",
    "\n",
    "class DeterministicActorNet(nn.Module, BasicNet):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 action_gate,\n",
    "                 action_scale,\n",
    "                 gpu=False,\n",
    "                 batch_norm=False,\n",
    "                 non_linear=F.relu):\n",
    "        super(DeterministicActorNet, self).__init__()\n",
    "\n",
    "        stride_time = state_dim[1] - 1 - 2 #\n",
    "        features = task.state_dim[0]\n",
    "        h0 = 2\n",
    "        h1 = 30\n",
    "        self.conv1 = nn.Conv2d(features, h0, (3, 1))\n",
    "        self.conv2 = nn.Conv2d(h0, h1, (stride_time, 1), stride=(stride_time, 1))\n",
    "        self.conv3 = nn.Conv2d((h1+1), 1, (1, 1))\n",
    "\n",
    "        self.action_scale = action_scale\n",
    "        self.action_gate = action_gate\n",
    "        self.non_linear = non_linear\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(h0)\n",
    "            self.bn2 = nn.BatchNorm1d(h1)\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        BasicNet.__init__(self, None, gpu, False)\n",
    "\n",
    "        self.tau = 0.005\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_torch_variable(x)\n",
    "        # print(\"shape of x: \", np.shape(x))\n",
    "        w0 = x[:,:1,:1,:] # weights from last step \n",
    "        x = x[:,:,1:,:]\n",
    "        \n",
    "        phi0 = self.non_linear(self.conv1(x))\n",
    "        if self.batch_norm:\n",
    "            phi0 = self.bn1(phi0)\n",
    "        phi1 = self.non_linear(self.conv2(phi0))\n",
    "        h = torch.cat([phi1,w0], 1)\n",
    "        if self.batch_norm:\n",
    "            h = self.bn2(h)\n",
    "        \n",
    "        action = self.conv3(h)\n",
    "        \n",
    "        # add cash_bias before we softmax\n",
    "        cash_bias_int = 0\n",
    "        cash_bias = self.to_torch_variable(torch.ones(action.size())[:,:,:,:1] * cash_bias_int)\n",
    "        action = torch.cat([cash_bias, action], -1)\n",
    "        \n",
    "        batch_size = action.size()[0]\n",
    "        action = action.view((batch_size,-1))\n",
    "        if self.action_gate:\n",
    "            action = self.action_scale * self.action_gate(action)\n",
    "        return action\n",
    "\n",
    "    def update(self, memory, critic, actor_t):\n",
    "\n",
    "            # Sample replay buffer\n",
    "            states, _, _, _, _ = memory.sample()\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -critic(states, self(states)).mean()\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.parameters(), actor_t.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def predict(self, x, to_numpy=True):\n",
    "        y = self.forward(x)\n",
    "        if to_numpy:\n",
    "            y = y.cpu().data.numpy()\n",
    "        return y\n",
    "\n",
    "    def set_params(self, params):\n",
    "        \"\"\"\n",
    "        Set the params of the network to the given parameters\n",
    "        \"\"\"\n",
    "        cpt = 0\n",
    "        for param in self.parameters():\n",
    "            tmp = np.product(param.size())\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                param.data.copy_(torch.from_numpy(\n",
    "                    params[cpt:cpt + tmp]).view(param.size()).cuda())\n",
    "            else:\n",
    "                param.data.copy_(torch.from_numpy(\n",
    "                    params[cpt:cpt + tmp]).view(param.size()))\n",
    "            cpt += tmp\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Returns parameters of the actor\n",
    "        \"\"\"\n",
    "        return deepcopy(np.hstack([to_numpy(v).flatten() for v in\n",
    "                                   self.parameters()]))\n",
    "\n",
    "    def get_grads(self):\n",
    "        \"\"\"\n",
    "        Returns the current gradient\n",
    "        \"\"\"\n",
    "        return deepcopy(np.hstack([to_numpy(v.grad).flatten() for v in self.parameters()]))\n",
    "\n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Returns the number of parameters of the network\n",
    "        \"\"\"\n",
    "        return self.get_params().shape[0]\n",
    "\n",
    "\n",
    "class DeterministicCriticNet(nn.Module, BasicNet):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 gpu=False,\n",
    "                 batch_norm=False,\n",
    "                 non_linear=F.relu,\n",
    "                 discount=0.99):\n",
    "        super(DeterministicCriticNet, self).__init__()\n",
    "        stride_time = state_dim[1] - 1 - 2 #\n",
    "        self.features = features = task.state_dim[0]\n",
    "        h0=2\n",
    "        h1=20\n",
    "        self.action = actions = action_dim -1\n",
    "        self.conv1 = nn.Conv2d(features, h0, (3, 1))\n",
    "        self.conv2 = nn.Conv2d(h0, h1, (stride_time, 1), stride=(stride_time, 1))\n",
    "        self.layer3 = nn.Linear((h1+2)*actions, 1)\n",
    "        self.non_linear = non_linear\n",
    "        self.discount = discount\n",
    "        self.tau = 0.005\n",
    "        #self.config = config\n",
    "        self.optimizer = config.critic_optimizer_fn(self.parameters())\n",
    "\n",
    "        if batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(h0)\n",
    "            self.bn2 = nn.BatchNorm1d(h1)\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        BasicNet.__init__(self, None, gpu, False)\n",
    "\n",
    "\n",
    "    def forward(self, x, action):\n",
    "        x = self.to_torch_variable(x)\n",
    "        action = self.to_torch_variable(action)[:,None,None,:-1] # remove cash bias\n",
    "        \n",
    "        w0 = x[:,:1,:1,:] # weights from last step \n",
    "        x = x[:,:,1:,:]\n",
    "        \n",
    "        phi0 = self.non_linear(self.conv1(x))\n",
    "        if self.batch_norm:\n",
    "            phi0 = self.bn1(phi0)\n",
    "        phi1 = self.non_linear(self.conv2(phi0))\n",
    "        h = torch.cat([phi1,w0,action], 1)\n",
    "        if self.batch_norm:\n",
    "            h = self.bn2(h)\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        action = self.layer3(h.view((batch_size,-1)))\n",
    "        return action\n",
    "\n",
    "    def update(self, memory, actor_t, critic_t):\n",
    "\n",
    "            # Sample replay buffer\n",
    "            states, actions, rewards, n_states, dones = memory.sample()\n",
    "\n",
    "            # Q target = reward + discount * Q(next_state, pi(next_state))\n",
    "            with torch.no_grad():\n",
    "                target_Q = critic_t(n_states, actor_t(n_states))\n",
    "                # print(\"original shape target_Q: \", np.shape(target_Q))\n",
    "                target_Q = target_Q * (1 - np.array([dones],dtype=np.float).reshape([64,1])) * np.array([self.discount],dtype=np.float) + np.array([rewards],dtype=np.float).reshape([64,1])\n",
    "                # print(target_Q.type())\n",
    "            # print(\"dones: \", type(dones))\n",
    "            # print(\"n_states:\", type(n_states))\n",
    "            # print(\"actions: \", type(actions))\n",
    "            # print(\"states: \", type(states))\n",
    "            # print(\"rewards: \", type(rewards))\n",
    "            # print(\"target_Q: \", type(target_Q))\n",
    "            # print()\n",
    "            # print(\"shape dones: \", np.shape(dones))\n",
    "            # print(\"shape n_states:\", np.shape(n_states))\n",
    "            # print(\"shape actions: \", np.shape(actions))\n",
    "            # print(\"shape states: \", np.shape(states))\n",
    "            # print(\"shape rewards: \", np.shape(rewards))\n",
    "            # print(\"shape target_Q: \", np.shape(target_Q))\n",
    "            # print()\n",
    "            # Get current Q estimates\n",
    "            current_Q = self(states, actions)\n",
    "            # print(current_Q.type())\n",
    "\n",
    "            # print(\"currentQ: \", current_Q)\n",
    "\n",
    "\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = nn.MSELoss()(current_Q, target_Q.float())\n",
    "\n",
    "            # Optimize the critic\n",
    "            self.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.parameters(), critic_t.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def predict(self, x, action):\n",
    "        return self.forward(x, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.task_fn = task_fn\n",
    "task = config.task_fn()\n",
    "config.actor_network_fn = lambda: DeterministicActorNet(\n",
    "    task.state_dim, task.action_dim, action_gate=None, action_scale=1.0, non_linear=F.relu, batch_norm=False, gpu=False)\n",
    "config.critic_network_fn = lambda: DeterministicCriticNet(\n",
    "    task.state_dim, task.action_dim, non_linear=F.relu, batch_norm=False, gpu=False)\n",
    "config.network_fn = lambda: DisjointActorCriticNet(config.actor_network_fn, config.critic_network_fn)\n",
    "config.actor_optimizer_fn = lambda params: torch.optim.Adam(params, lr=4e-5)\n",
    "config.critic_optimizer_fn =    lambda params: torch.optim.Adam(params, lr=5e-4, weight_decay=0.001)\n",
    "config.replay_fn = lambda: HighDimActionReplay(memory_size=600, batch_size=64)\n",
    "config.random_process_fn =     lambda: OrnsteinUhlenbeckProcess(size=task.action_dim, theta=0.15, sigma=0.2, sigma_min=0.00002, n_steps_annealing=10000)\n",
    "config.discount = 0.0\n",
    "\n",
    "config.min_memory_size = 50\n",
    "config.target_network_mix = 0.001\n",
    "config.max_episode_length = 3000  \n",
    "config.target_network_mix = 0.01\n",
    "config.noise_decay_interval = 100000\n",
    "config.gradient_clip = 20\n",
    "config.min_epsilon = 0.1\n",
    "\n",
    "\n",
    "# sigma_init = 1e-3\n",
    "# damp = 1e-3\n",
    "# damp_limit = 1e-5\n",
    "# pop_size = 10\n",
    "# elitism = 'elitism'\n",
    "\n",
    "\n",
    "# Many papers have found rewards scaling to be an important parameter. But while they focus on the scaling factor\n",
    "# I think they should focus on the end variance with a range of 200-400. e.g. https://arxiv.org/pdf/1709.06560.pdf\n",
    "# Hard to tell for sure without experiments to prove it\n",
    "config.reward_scaling = 1000\n",
    "\n",
    "# config.test_interval = 10 # ORIGINALLY\n",
    "config.test_interval = 2 # TODO: Remove (quick test)\n",
    "config.test_repetitions = 1\n",
    "# config.save_interval = 40 # ORIGINALLY\n",
    "config.save_interval = 4 # TODO: Remove (quick test)\n",
    "config.logger = Logger('./log', gym.logger)\n",
    "config.tag = tag\n",
    "\n",
    "\n",
    "agent = DDPGAgent(config)\n",
    "agent\n",
    "\n",
    "# es = sepCEM(agent.worker_network.actor.get_size(), mu_init=agent.worker_network.actor.get_params(), sigma_init=sigma_init, damp=damp, damp_limit=damp_limit,\n",
    "#             pop_size=pop_size, antithetic=not pop_size % 2, parents=pop_size // 2, elitism=elitism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepRL.utils import run_episodes\n",
    "agent.task._plot = agent.task._plot2 = None\n",
    "try:    \n",
    "    run_episodes(agent)\n",
    "except KeyboardInterrupt as e:\n",
    "    save_ddpg(agent)\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df_online, df = load_stats_ddpg(agent)\n",
    "sns.regplot(x=\"step\", y=\"rewards\", data=df_online, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly growth\n",
    "portfolio_return = (1+df_online.rewards[-100:].mean())\n",
    "\n",
    "returns = task.unwrapped.src.data[0,:,:1]\n",
    "market_return = (1+returns).mean()\n",
    "market_return, portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_algo(env, algo, seed=0):\n",
    "    \"\"\"\n",
    "    Runs and algo from https://github.com/Marigold/universal-portfolios on env\n",
    "    \n",
    "    https://github.com/Marigold/universal-portfolios/commit/e8970a82427522ef11b1c3cbf681e18b5fe8169c\n",
    "    \"\"\"\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    state = env.reset()\n",
    "    for i in range(env.unwrapped.sim.steps):\n",
    "        \n",
    "        history= pd.DataFrame(state[0,:,:], columns=env.unwrapped.src.asset_names)\n",
    "        # MPT wants a cash column, and it should be first\n",
    "        history['CASH']=1\n",
    "        history=history[['CASH'] + env.unwrapped.src.asset_names]\n",
    "#         cols = list(history.columns)\n",
    "#         cols[0]='CASH'\n",
    "#         history.columns = cols\n",
    "        \n",
    "        x=history.iloc[-1]\n",
    "        \n",
    "        last_b = env.unwrapped.sim.w0#[1:]\n",
    "\n",
    "        algo.init_step(history)\n",
    "        # some don't want history\n",
    "        try:\n",
    "            action = algo.step(x, last_b, history)\n",
    "        except TypeError:\n",
    "            action = algo.step(x, last_b)\n",
    "        \n",
    "        # might by dataframe\n",
    "        action = getattr(action, 'value', action)\n",
    "        \n",
    "        # For upt\n",
    "        if isinstance(action, np.matrixlib.defmatrix.matrix):\n",
    "            action = np.array(action.tolist()).T[0]\n",
    "            \n",
    "        \n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break   \n",
    "    df = pd.DataFrame(env.unwrapped.infos)\n",
    "    df.index = pd.to_datetime(df['date']*1e9)\n",
    "    return df['portfolio_value'], df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test env\n",
    "df_test = pd.read_hdf('./data/poloniex_30m.hf',key='test')\n",
    "test_steps=100 #5000\n",
    "env_test = task_fn_test()\n",
    "agent.task = env_test\n",
    "agent.config.max_episode_length = test_steps\n",
    "agent.task.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "# run in deterministic mode, no training, no exploration\n",
    "agent.episode(True)\n",
    "# agent.task.render('notebook')\n",
    "# agent.task.render('notebook', True)\n",
    "\n",
    "df = pd.DataFrame(agent.task.unwrapped.infos)\n",
    "df.index = pd.to_datetime(df['date']*1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('tsol3y': virtualenv)",
   "language": "python",
   "name": "python36564bittsol3yvirtualenv97b3b979af074740b729842f805d38b0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
